{
	"version": "https://jsonfeed.org/version/1.1",
	"title": "Gaetan De Waele",
	"language": "en",
	"home_page_url": "https://gdewael.github.io/",
	"feed_url": "https://gdewael.github.io/feed/feed.json",
	"description": "Gaetan De Waele&#39;s personal site",
	"author": {
		"name": "Gaetan De Waele",
		"url": "https://gdewael.github.io "
	},
	"items": [
		{
			"id": "https://gdewael.github.io/blog/flashattnvarlen/",
			"url": "https://gdewael.github.io/blog/flashattnvarlen/",
			"title": "Hacking &quot;vanilla&quot; FlashAttention for variable-length inputs",
			"content_html": "<p>This blogpost concerns anyone who (1) is dealing with variable-length data samples, and (2) is looking to optimize their transformer-based code.</p>\n<hr>\n<p><a href=\"https://github.com/Dao-AILab/flash-attention\">FlashAttention</a> has changed the transformer game for some two years<sup class=\"footnote-ref\"><a href=\"https://gdewael.github.io/blog/flashattnvarlen/\" id=\"fnref1\">[1]</a></sup>.\nCompared to naive self-attention implementations, it boasts <a href=\"https://github.com/Dao-AILab/flash-attention/blob/main/assets/flash3_fp16_fwd.png\">staggering speed gains</a> in forward passes -- depending on the input sequence lengths.\nIn addition, it can reduce the <a href=\"https://github.com/Dao-AILab/flash-attention/blob/main/assets/flashattn_memory.jpg\">VRAM footprint</a> of self-attention by literal orders of magnitude as it reduces the <mjx-container class=\"MathJax\" jax=\"SVG\" style=\"direction: ltr; position: relative;\"><svg style=\"overflow: visible; min-height: 1px; min-width: 1px; vertical-align: -0.566ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"5.906ex\" height=\"2.452ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -833.9 2610.6 1083.9\" aria-hidden=\"true\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"TeXAtom\" data-mjx-texclass=\"ORD\"><g data-mml-node=\"mi\"><path data-c=\"4F\" d=\"M308 428Q289 428 289 438Q289 457 318 508T378 593Q417 638 475 671T599 705Q688 705 732 643T777 483Q777 380 733 285T620 123T464 18T293 -22Q188 -22 123 51T58 245Q58 327 87 403T159 533T249 626T333 685T388 705Q404 705 404 693Q404 674 363 649Q333 632 304 606T239 537T181 429T158 290Q158 179 214 114T364 48Q489 48 583 165T677 438Q677 473 670 505T648 568T601 617T528 636Q518 636 513 635Q486 629 460 600T419 544T392 490Q383 470 372 459Q341 430 308 428Z\" style=\"stroke-width: 3;\"></path></g></g><g data-mml-node=\"mo\" transform=\"translate(796,0)\"><path data-c=\"28\" d=\"M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z\" style=\"stroke-width: 3;\"></path></g><g data-mml-node=\"msup\" transform=\"translate(1185,0)\"><g data-mml-node=\"mi\"><path data-c=\"1D45B\" d=\"M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z\" style=\"stroke-width: 3;\"></path></g><g data-mml-node=\"mn\" transform=\"translate(633,363) scale(0.707)\"><path data-c=\"32\" d=\"M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z\" style=\"stroke-width: 3;\"></path></g></g><g data-mml-node=\"mo\" transform=\"translate(2221.6,0)\"><path data-c=\"29\" d=\"M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z\" style=\"stroke-width: 3;\"></path></g></g></g></svg><mjx-assistive-mml unselectable=\"on\" display=\"inline\" style=\"top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow data-mjx-texclass=\"ORD\"><mi data-mjx-variant=\"-tex-calligraphic\" mathvariant=\"script\">O</mi></mrow><mo stretchy=\"false\">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo></math></mjx-assistive-mml></mjx-container> self-attention memory footprint to <mjx-container class=\"MathJax\" jax=\"SVG\" style=\"direction: ltr; position: relative;\"><svg style=\"overflow: visible; min-height: 1px; min-width: 1px; vertical-align: -0.566ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"4.919ex\" height=\"2.262ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -750 2174 1000\" aria-hidden=\"true\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"TeXAtom\" data-mjx-texclass=\"ORD\"><g data-mml-node=\"mi\"><path data-c=\"4F\" d=\"M308 428Q289 428 289 438Q289 457 318 508T378 593Q417 638 475 671T599 705Q688 705 732 643T777 483Q777 380 733 285T620 123T464 18T293 -22Q188 -22 123 51T58 245Q58 327 87 403T159 533T249 626T333 685T388 705Q404 705 404 693Q404 674 363 649Q333 632 304 606T239 537T181 429T158 290Q158 179 214 114T364 48Q489 48 583 165T677 438Q677 473 670 505T648 568T601 617T528 636Q518 636 513 635Q486 629 460 600T419 544T392 490Q383 470 372 459Q341 430 308 428Z\" style=\"stroke-width: 3;\"></path></g></g><g data-mml-node=\"mo\" transform=\"translate(796,0)\"><path data-c=\"28\" d=\"M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z\" style=\"stroke-width: 3;\"></path></g><g data-mml-node=\"mi\" transform=\"translate(1185,0)\"><path data-c=\"1D45B\" d=\"M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z\" style=\"stroke-width: 3;\"></path></g><g data-mml-node=\"mo\" transform=\"translate(1785,0)\"><path data-c=\"29\" d=\"M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z\" style=\"stroke-width: 3;\"></path></g></g></g></svg><mjx-assistive-mml unselectable=\"on\" display=\"inline\" style=\"top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow data-mjx-texclass=\"ORD\"><mi data-mjx-variant=\"-tex-calligraphic\" mathvariant=\"script\">O</mi></mrow><mo stretchy=\"false\">(</mo><mi>n</mi><mo stretchy=\"false\">)</mo></math></mjx-assistive-mml></mjx-container>.\nBesides allowing bigger input sequence lengths, lower memory footprints allows people to train larger models with larger batch sizes using the same compute budget.\nAs FlashAttention just represents an optimized kernel, it does all this without any of the significant drawbacks that attention approximations exhibit.</p>\n<p>As it's since been integrated into the main PyTorch library, it's usage is quickly becoming the standard MO for self-attention computation.</p>\n<h2 id=\"underperformance-of-flash-attn-varlen-func-vs-vanilla-flashattention\" tabindex=\"-1\">Underperformance of <code>flash_attn_varlen_func</code> vs &quot;vanilla&quot; FlashAttention <a class=\"header-anchor\" href=\"https://gdewael.github.io/blog/flashattnvarlen/\">#</a></h2>\n<p>If you have variable-length sequence data, the usual thing to do is to provide a <mjx-container class=\"MathJax\" jax=\"SVG\" style=\"direction: ltr; position: relative;\"><svg style=\"overflow: visible; min-height: 1px; min-width: 1px; vertical-align: 0;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"6.315ex\" height=\"1.545ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -683 2791.4 683\" aria-hidden=\"true\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"mi\"><path data-c=\"1D441\" d=\"M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z\" style=\"stroke-width: 3;\"></path></g><g data-mml-node=\"mo\" transform=\"translate(1110.2,0)\"><path data-c=\"D7\" d=\"M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z\" style=\"stroke-width: 3;\"></path></g><g data-mml-node=\"mi\" transform=\"translate(2110.4,0)\"><path data-c=\"1D43F\" d=\"M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z\" style=\"stroke-width: 3;\"></path></g></g></g></svg><mjx-assistive-mml unselectable=\"on\" display=\"inline\" style=\"top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>N</mi><mo>×</mo><mi>L</mi></math></mjx-assistive-mml></mjx-container> boolean mask, indicating where the padding tokens are.\nThis mask will restrict self-attention to not let padded tokens participate.\nAs it currently stands, FlashAttention does not support such masks.\nRather, it provides separate implementations for &quot;vanilla&quot; self-attention and &quot;variable-length&quot; self-attention.\n(If you use the main PyTorch self-attention implementation and provide a mask to account for variable lengths, PyTorch will use a slower non-FlashAttention implementation instead).</p>\n<p>The problem is that FlashAttention's variable-length operation simply does not deliver the same throughputs as the default operation for smaller-to-moderate sequence lengths:</p>\n<div style=\"text-align: center;\">\n<img src=\"https://gdewael.github.io/blog/flashattnvarlen/benchmark.svg\" alt=\"varlendata\" style=\"max-height: 700px; max-width: 700px;\">\n</div>\n<p>Benchmark details can be found in the footnotes<sup class=\"footnote-ref\"><a href=\"https://gdewael.github.io/blog/flashattnvarlen/\" id=\"fnref2\">[2]</a></sup>.</p>\n<p>In order to circumvent the usage of FlashAttention's varlen operation, I propose to hack the data in a way such that - per batch - it is not variable-length anymore.</p>\n<h2 id=\"cutting-to-min-size-in-each-batch\" tabindex=\"-1\">Cutting to min size in each batch <a class=\"header-anchor\" href=\"https://gdewael.github.io/blog/flashattnvarlen/\">#</a></h2>\n<p>Consider a dataset cut into batches.\nThe simplest way in order to hack the data to fixed-length per batch is to cut off the ends of the data per batch to the smallest size:</p>\n<div style=\"text-align: center;\">\n<img src=\"https://gdewael.github.io/blog/flashattnvarlen/varlendata.svg\" alt=\"varlendata\" style=\"max-height: 350px; max-width: 350px;\">\n</div>\n<p>In PyTorch, one can achieve this by (1) wrapping a Dataset object to eliminate any padding to the original sequence length size, and (2) writing a custom batch collate function that aggregates all the samples in a batch and cuts to the minimal sequence length.\nA simple implementation could look something like this:</p>\n<pre class=\"language-python\" tabindex=\"0\"><code class=\"language-python\"><span class=\"token keyword\">from</span> torch<span class=\"token punctuation\">.</span>utils<span class=\"token punctuation\">.</span>data <span class=\"token keyword\">import</span> <span class=\"token operator\">*</span>\n<span class=\"token keyword\">import</span> torch\n\n<span class=\"token keyword\">class</span> <span class=\"token class-name\">Cut2MinDatasetWrapper</span><span class=\"token punctuation\">(</span>Dataset<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__init__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> dataset<span class=\"token punctuation\">,</span> seqlens<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token builtin\">super</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>__init__<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>dataset <span class=\"token operator\">=</span> dataset\n        self<span class=\"token punctuation\">.</span>seqlens <span class=\"token operator\">=</span> seqlens\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__getitem__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> index<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        sample <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>dataset<span class=\"token punctuation\">[</span>index<span class=\"token punctuation\">]</span>\n        seqlen <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>seqlens<span class=\"token punctuation\">[</span>index<span class=\"token punctuation\">]</span>\n        <span class=\"token keyword\">return</span> sample<span class=\"token punctuation\">[</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">:</span>seqlen<span class=\"token punctuation\">]</span> <span class=\"token comment\"># eliminates padding</span>\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__len__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">return</span> <span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>dataset<span class=\"token punctuation\">)</span>\n    \n    <span class=\"token keyword\">def</span> <span class=\"token function\">collate_fn</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> batch<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">return</span> default_collate<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>_cut_to_uniform_size<span class=\"token punctuation\">(</span>batch<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token decorator annotation punctuation\">@staticmethod</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">_cut_to_uniform_size</span><span class=\"token punctuation\">(</span>list_of_objects<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        min_len <span class=\"token operator\">=</span> <span class=\"token builtin\">min</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>b<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">[</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span> <span class=\"token keyword\">for</span> b <span class=\"token keyword\">in</span> list_of_objects<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">return</span> <span class=\"token punctuation\">[</span>b<span class=\"token punctuation\">[</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">:</span>min_len<span class=\"token punctuation\">]</span> <span class=\"token keyword\">for</span> b <span class=\"token keyword\">in</span> list_of_objects<span class=\"token punctuation\">]</span></code></pre>\n<p>For an example usage, consider a dummy dataset of 10000 variable-length tensors (ranging from 5 to 100 in length):</p>\n<pre class=\"language-python\" tabindex=\"0\"><code class=\"language-python\">dataset <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>\n<span class=\"token keyword\">for</span> _ <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span><span class=\"token number\">10000</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    dataset<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>torch<span class=\"token punctuation\">.</span>randn<span class=\"token punctuation\">(</span>torch<span class=\"token punctuation\">.</span>randint<span class=\"token punctuation\">(</span>size<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> low<span class=\"token operator\">=</span><span class=\"token number\">5</span><span class=\"token punctuation\">,</span> high<span class=\"token operator\">=</span><span class=\"token number\">1000</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\nseqlens <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>tensor<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>sample<span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span> sample <span class=\"token keyword\">in</span> dataset<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\ndataset <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>utils<span class=\"token punctuation\">.</span>rnn<span class=\"token punctuation\">.</span>pad_sequence<span class=\"token punctuation\">(</span>dataset<span class=\"token punctuation\">,</span> batch_first<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span></code></pre>\n<p>Usage of this dataset object would look like this:</p>\n<pre class=\"language-python\" tabindex=\"0\"><code class=\"language-python\">dataset <span class=\"token operator\">=</span> Cut2MinDatasetWrapper<span class=\"token punctuation\">(</span>dataset<span class=\"token punctuation\">,</span> seqlens<span class=\"token punctuation\">)</span>\ndataloader <span class=\"token operator\">=</span> DataLoader<span class=\"token punctuation\">(</span>\n    dataset<span class=\"token punctuation\">,</span>\n    batch_size<span class=\"token operator\">=</span><span class=\"token number\">8</span><span class=\"token punctuation\">,</span>\n    shuffle<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">,</span>\n    collate_fn<span class=\"token operator\">=</span>dataset<span class=\"token punctuation\">.</span>collate_fn\n<span class=\"token punctuation\">)</span>\n<span class=\"token builtin\">next</span><span class=\"token punctuation\">(</span><span class=\"token builtin\">iter</span><span class=\"token punctuation\">(</span>dataloader<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre>\n<h2 id=\"minimizing-data-loss-by-bucket-batching\" tabindex=\"-1\">Minimizing data loss by bucket batching <a class=\"header-anchor\" href=\"https://gdewael.github.io/blog/flashattnvarlen/\">#</a></h2>\n<p>It's easy to see that we will merit somewhat by making sure similar sizes are batched together.\nThis is where bucket sampling, a concept which has been around for quite some time, comes in.\nFor some reason, it was impossible for me to find a modern implementation (that includes support for - for example - distributed data parallel), so here I will provide a simple implementation:</p>\n<pre class=\"language-python\" tabindex=\"0\"><code class=\"language-python\"><span class=\"token keyword\">def</span> <span class=\"token function\">_partitions_to_len</span><span class=\"token punctuation\">(</span>n_samples<span class=\"token punctuation\">,</span> n_partitions<span class=\"token punctuation\">,</span> batch_size<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token comment\"># Count the number of samples per partition</span>\n        samples_per_partition <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>\n            math<span class=\"token punctuation\">.</span>ceil<span class=\"token punctuation\">(</span>n_samples <span class=\"token operator\">/</span> n_partitions<span class=\"token punctuation\">)</span>\n        <span class=\"token punctuation\">]</span> <span class=\"token operator\">*</span> n_partitions\n\n        <span class=\"token comment\"># The last partition may have fewer samples</span>\n        samples_per_partition<span class=\"token punctuation\">[</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">-=</span> <span class=\"token punctuation\">(</span>n_samples <span class=\"token operator\">//</span> n_partitions<span class=\"token punctuation\">)</span> <span class=\"token operator\">%</span> n_partitions\n\n        <span class=\"token comment\"># Count the number of batches per partition and sum</span>\n        len_ <span class=\"token operator\">=</span> <span class=\"token builtin\">sum</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>math<span class=\"token punctuation\">.</span>ceil<span class=\"token punctuation\">(</span>samples <span class=\"token operator\">/</span> batch_size<span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span> samples <span class=\"token keyword\">in</span> samples_per_partition<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">return</span> len_\n\n<span class=\"token keyword\">class</span> <span class=\"token class-name\">BucketBatchSampler</span><span class=\"token punctuation\">(</span>BatchSampler<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__init__</span><span class=\"token punctuation\">(</span>\n        self<span class=\"token punctuation\">,</span>\n        dataset<span class=\"token punctuation\">,</span>\n        seqlens<span class=\"token punctuation\">,</span> <span class=\"token comment\"># torch.Tensor (n, )</span>\n        batch_size<span class=\"token punctuation\">,</span>\n        n_partitions<span class=\"token operator\">=</span><span class=\"token number\">100</span><span class=\"token punctuation\">,</span>\n        indices<span class=\"token operator\">=</span><span class=\"token boolean\">None</span><span class=\"token punctuation\">,</span> <span class=\"token comment\"># None or list</span>\n        drop_last<span class=\"token operator\">=</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">,</span>\n    <span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token builtin\">super</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>__init__<span class=\"token punctuation\">(</span>dataset<span class=\"token punctuation\">,</span> batch_size<span class=\"token punctuation\">,</span> drop_last<span class=\"token punctuation\">)</span>\n\n        <span class=\"token comment\"># `indices` subsamples the dataset in the case of a Distributed Data setting</span>\n        <span class=\"token keyword\">if</span> indices <span class=\"token keyword\">is</span> <span class=\"token keyword\">not</span> <span class=\"token boolean\">None</span><span class=\"token punctuation\">:</span>\n            len_dataset <span class=\"token operator\">=</span> <span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>indices<span class=\"token punctuation\">)</span>\n            self<span class=\"token punctuation\">.</span>seqlens <span class=\"token operator\">=</span> seqlens<span class=\"token punctuation\">[</span>indices<span class=\"token punctuation\">]</span>\n            indices <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>tensor<span class=\"token punctuation\">(</span>indices<span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">else</span><span class=\"token punctuation\">:</span>\n            len_dataset <span class=\"token operator\">=</span> <span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>dataset<span class=\"token punctuation\">)</span>\n            self<span class=\"token punctuation\">.</span>seqlens <span class=\"token operator\">=</span> seqlens\n            indices <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>arange<span class=\"token punctuation\">(</span>len_dataset<span class=\"token punctuation\">)</span>\n\n        <span class=\"token comment\"># randomly partition dataset in n_partitions</span>\n        self<span class=\"token punctuation\">.</span>partitioner <span class=\"token operator\">=</span> BatchSampler<span class=\"token punctuation\">(</span>\n            RandomSampler<span class=\"token punctuation\">(</span>indices<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n            math<span class=\"token punctuation\">.</span>ceil<span class=\"token punctuation\">(</span>len_dataset <span class=\"token operator\">/</span> n_partitions<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n            <span class=\"token boolean\">False</span>\n        <span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>indices <span class=\"token operator\">=</span> indices\n\n        self<span class=\"token punctuation\">.</span>_len <span class=\"token operator\">=</span> _partitions_to_len<span class=\"token punctuation\">(</span>len_dataset<span class=\"token punctuation\">,</span> n_partitions<span class=\"token punctuation\">,</span> batch_size<span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__iter__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token comment\"># For every partition, order all indices in it by seq. len</span>\n        indices_per_partition_ordered <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>\n        <span class=\"token keyword\">for</span> partition <span class=\"token keyword\">in</span> self<span class=\"token punctuation\">.</span>partitioner<span class=\"token punctuation\">:</span>\n            partition_indices <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>indices<span class=\"token punctuation\">[</span>partition<span class=\"token punctuation\">]</span>\n\n            partition_asort_seqlens <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>argsort<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>seqlens<span class=\"token punctuation\">[</span>partition<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> descending<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span>\n            partition_indices_in_order <span class=\"token operator\">=</span> <span class=\"token builtin\">list</span><span class=\"token punctuation\">(</span>partition_indices<span class=\"token punctuation\">[</span>partition_asort_seqlens<span class=\"token punctuation\">.</span>numpy<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n            indices_per_partition_ordered<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>partition_indices_in_order<span class=\"token punctuation\">)</span>\n\n        <span class=\"token comment\"># Then iterate through all partitions</span>\n        <span class=\"token keyword\">for</span> partition_indices <span class=\"token keyword\">in</span> indices_per_partition_ordered<span class=\"token punctuation\">:</span>\n            <span class=\"token comment\"># Make batches per partition, then randomly shuffle around</span>\n            <span class=\"token comment\"># The shuffling prevents that the smallest batches will always be first</span>\n            <span class=\"token keyword\">for</span> batch <span class=\"token keyword\">in</span> SubsetRandomSampler<span class=\"token punctuation\">(</span><span class=\"token builtin\">list</span><span class=\"token punctuation\">(</span>BatchSampler<span class=\"token punctuation\">(</span>partition_indices<span class=\"token punctuation\">,</span> self<span class=\"token punctuation\">.</span>batch_size<span class=\"token punctuation\">,</span> self<span class=\"token punctuation\">.</span>drop_last<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n                <span class=\"token keyword\">yield</span> batch\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__len__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">return</span> self<span class=\"token punctuation\">.</span>_len</code></pre>\n<p>The following schematic should make it clear what is happening:</p>\n<div style=\"text-align: center;\">\n<img src=\"https://gdewael.github.io/blog/flashattnvarlen/bucketbatch.svg\" alt=\"bucketbatch\" style=\"max-height: 850px; max-width: 850px;\">\n</div>\n<p>Note that this implementation includes many steps in order to retain as much &quot;stochasticity&quot; as possible in batch construction.\nPartitioning the data in subsets before sorting makes it so that samples do not consistently land in the same batch each epoch.\nShuffling batches after bucketing makes sure that the model is not consistently presented with batches of similar sizes right after one another.</p>\n<p>The distributed data parallel version of bucket sampling follows the same procedure - but separately for each slice of data each device processes in an epoch. Implementation in the footnotes<sup class=\"footnote-ref\"><a href=\"https://gdewael.github.io/blog/flashattnvarlen/\" id=\"fnref3\">[3]</a></sup>.</p>\n<p>An example using the same data as previous:</p>\n<pre class=\"language-python\" tabindex=\"0\"><code class=\"language-python\">sampler <span class=\"token operator\">=</span> BucketBatchSampler<span class=\"token punctuation\">(</span>\n    dataset<span class=\"token punctuation\">,</span>\n    seqlens<span class=\"token punctuation\">,</span>\n    batch_size<span class=\"token operator\">=</span><span class=\"token number\">8</span><span class=\"token punctuation\">,</span>\n    n_partitions<span class=\"token operator\">=</span><span class=\"token number\">20</span><span class=\"token punctuation\">,</span>\n    <span class=\"token punctuation\">)</span>\n\ndataloader <span class=\"token operator\">=</span> DataLoader<span class=\"token punctuation\">(</span>\n    dataset<span class=\"token punctuation\">,</span>\n    batch_sampler<span class=\"token operator\">=</span>sampler<span class=\"token punctuation\">,</span>\n    collate_fn<span class=\"token operator\">=</span>dataset<span class=\"token punctuation\">.</span>collate_fn\n<span class=\"token punctuation\">)</span>\n<span class=\"token builtin\">next</span><span class=\"token punctuation\">(</span><span class=\"token builtin\">iter</span><span class=\"token punctuation\">(</span>dataloader<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre>\n<p>Using the previous dummy example and <strong>without bucket batching,</strong> the proportion of tokens thrown away due to cutting to min size in a batch is <strong><mjx-container class=\"MathJax\" jax=\"SVG\" style=\"direction: ltr; position: relative;\"><svg style=\"overflow: visible; min-height: 1px; min-width: 1px; vertical-align: 0;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"1.76ex\" height=\"1.507ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -666 778 666\" aria-hidden=\"true\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"mo\"><path data-c=\"B1\" d=\"M56 320T56 333T70 353H369V502Q369 651 371 655Q376 666 388 666Q402 666 405 654T409 596V500V353H707Q722 345 722 333Q722 320 707 313H409V40H707Q722 32 722 20T707 0H70Q56 7 56 20T70 40H369V313H70Q56 320 56 333Z\" style=\"stroke-width: 3;\"></path></g></g></g></svg><mjx-assistive-mml unselectable=\"on\" display=\"inline\" style=\"top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo>±</mo></math></mjx-assistive-mml></mjx-container> 77%</strong>.\n<strong>With bucket batching as above, this figure becomes <mjx-container class=\"MathJax\" jax=\"SVG\" style=\"direction: ltr; position: relative;\"><svg style=\"overflow: visible; min-height: 1px; min-width: 1px; vertical-align: 0;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"1.76ex\" height=\"1.507ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -666 778 666\" aria-hidden=\"true\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"mo\"><path data-c=\"B1\" d=\"M56 320T56 333T70 353H369V502Q369 651 371 655Q376 666 388 666Q402 666 405 654T409 596V500V353H707Q722 345 722 333Q722 320 707 313H409V40H707Q722 32 722 20T707 0H70Q56 7 56 20T70 40H369V313H70Q56 320 56 333Z\" style=\"stroke-width: 3;\"></path></g></g></g></svg><mjx-assistive-mml unselectable=\"on\" display=\"inline\" style=\"top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo>±</mo></math></mjx-assistive-mml></mjx-container> 1.39%</strong>.</p>\n<p>Note that the number of made partitions has an impact on this figure.\nIncreasing the partition number makes mini-batch construction more random, but each subset is smaller.\nThe smaller size of each subset makes it so that it is harder to create batches wherein all samples have similar sequence lengths.\nHence, it is crucial to balance the number of partitions with training dataset size.</p>\n<p>Many problem settings allow the remaining data-token loss to be made as inconsequential as possible.\nFor example, with many data modalities, one can meaningfully sort the tokens in a sample such that the least-important tokens are last in the input set.\nConsider scRNA-seq, where one might only input non-zero counts into the model, such as in scGPT<sup class=\"footnote-ref\"><a href=\"https://gdewael.github.io/blog/flashattnvarlen/\" id=\"fnref4\">[4]</a></sup>.\nIf one sorts gene inputs by its count value, only the very-lowly expressed genes are thrown away.\nFor mass spectral data, we can construct a similar rationale, throwing away the lowest intensity peaks in a spectrum.\nFor protein/RNA/DNA/SMILES sequences, I see two choices: either (1) similarly cutting off the ends, or (2) taking a random crop.</p>\n<p>Using this framework, every batch has the same number of tokens, relinquishing the need for masking tokens.\nHence, it is possible to use &quot;vanilla&quot; FlashAttention operation again.</p>\n<h2 id=\"a-pypi-package\" tabindex=\"-1\">A PyPI package <a class=\"header-anchor\" href=\"https://gdewael.github.io/blog/flashattnvarlen/\">#</a></h2>\n<p>If you want to use the concepts I've laid out here for yourself, I have deposited a more fleshed-out version of the code on <a href=\"https://github.com/gdewael/cut2min-bucket\">GitHub</a>.\nAdditionally, the code is distributed on PyPI as a (hopefully) easy to use package:</p>\n<pre class=\"language-bash\" tabindex=\"0\"><code class=\"language-bash\">pip <span class=\"token function\">install</span> cut2min-bucket</code></pre>\n<hr><h2 class=\"mt-3\">References and footnotes</h2>\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn1\" class=\"footnote-item\"><p>Dao, Tri, et al. &quot;Flashattention: Fast and memory-efficient exact attention with io-awareness.&quot; Advances in Neural Information Processing Systems 35 (2022): 16344-16359. <a href=\"https://gdewael.github.io/blog/flashattnvarlen/\" class=\"footnote-backref\">↩︎</a></p>\n</li>\n<li id=\"fn2\" class=\"footnote-item\"><p>To reproduce the <code>flash_attn_varlen_func</code> forward speed benchmark, using <code>torch 2.4.0</code> and <code>flash-attn 2.6.3</code>, run in an IPython Notebook:</p>\n<pre class=\"language-python\" tabindex=\"0\"><code class=\"language-python\">SEQLEN <span class=\"token operator\">=</span> <span class=\"token number\">512</span>\n<span class=\"token keyword\">import</span> torch\n<span class=\"token keyword\">import</span> torch<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>functional <span class=\"token keyword\">as</span> F\n<span class=\"token keyword\">from</span> flash_attn <span class=\"token keyword\">import</span> flash_attn_func<span class=\"token punctuation\">,</span> flash_attn_varlen_func\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">modified_forward_default</span><span class=\"token punctuation\">(</span>q<span class=\"token punctuation\">,</span>k<span class=\"token punctuation\">,</span>v<span class=\"token punctuation\">,</span> mask<span class=\"token operator\">=</span><span class=\"token boolean\">None</span><span class=\"token punctuation\">,</span> causal<span class=\"token operator\">=</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">return</span> flash_attn_func<span class=\"token punctuation\">(</span>q<span class=\"token punctuation\">,</span>k<span class=\"token punctuation\">,</span>v<span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">modified_forward_varlen</span><span class=\"token punctuation\">(</span>q<span class=\"token punctuation\">,</span>k<span class=\"token punctuation\">,</span>v<span class=\"token punctuation\">,</span> mask<span class=\"token operator\">=</span><span class=\"token boolean\">None</span><span class=\"token punctuation\">,</span> causal<span class=\"token operator\">=</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    bsz<span class=\"token punctuation\">,</span> seqlen<span class=\"token punctuation\">,</span> nh<span class=\"token punctuation\">,</span> h <span class=\"token operator\">=</span> q<span class=\"token punctuation\">.</span>shape\n    q <span class=\"token operator\">=</span> q<span class=\"token punctuation\">.</span>view<span class=\"token punctuation\">(</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> nh<span class=\"token punctuation\">,</span> h<span class=\"token punctuation\">)</span>\n    k <span class=\"token operator\">=</span> k<span class=\"token punctuation\">.</span>view<span class=\"token punctuation\">(</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> nh<span class=\"token punctuation\">,</span> h<span class=\"token punctuation\">)</span>\n    v <span class=\"token operator\">=</span> v<span class=\"token punctuation\">.</span>view<span class=\"token punctuation\">(</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> nh<span class=\"token punctuation\">,</span> h<span class=\"token punctuation\">)</span>\n    seqlens <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>arange<span class=\"token punctuation\">(</span>bsz<span class=\"token operator\">+</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">*</span> seqlen\n    seqlens_pert <span class=\"token operator\">=</span> seqlens <span class=\"token operator\">+</span> F<span class=\"token punctuation\">.</span>pad<span class=\"token punctuation\">(</span>torch<span class=\"token punctuation\">.</span>tensor<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>repeat<span class=\"token punctuation\">(</span>bsz<span class=\"token operator\">//</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>cumsum<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    <span class=\"token comment\"># we periodically assign 1 tokens more or less from and to each sequence</span>\n    <span class=\"token comment\"># this makes `flash_attn_varlen_func` handle the input as variable length</span>\n    <span class=\"token comment\"># otherwise, it shortcuts to default flash attention</span>\n    cu_seqlens_q <span class=\"token operator\">=</span> cu_seqlens_k <span class=\"token operator\">=</span> seqlens_pert<span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span>q<span class=\"token punctuation\">.</span>device<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span>torch<span class=\"token punctuation\">.</span>int32<span class=\"token punctuation\">)</span>\n    max_seqlen_q <span class=\"token operator\">=</span> max_seqlen_k <span class=\"token operator\">=</span> seqlen<span class=\"token operator\">+</span><span class=\"token number\">2</span>\n    <span class=\"token keyword\">return</span> flash_attn_varlen_func<span class=\"token punctuation\">(</span>q<span class=\"token punctuation\">,</span>k<span class=\"token punctuation\">,</span>v<span class=\"token punctuation\">,</span> cu_seqlens_q<span class=\"token punctuation\">,</span> cu_seqlens_k<span class=\"token punctuation\">,</span> max_seqlen_q<span class=\"token punctuation\">,</span> max_seqlen_k<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>view<span class=\"token punctuation\">(</span>bsz<span class=\"token punctuation\">,</span> seqlen<span class=\"token punctuation\">,</span> nh<span class=\"token punctuation\">,</span> h<span class=\"token punctuation\">)</span>\n\n\nq <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>randn<span class=\"token punctuation\">(</span><span class=\"token number\">16</span><span class=\"token punctuation\">,</span> SEQLEN<span class=\"token punctuation\">,</span> <span class=\"token number\">8</span><span class=\"token punctuation\">,</span> <span class=\"token number\">32</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span>torch<span class=\"token punctuation\">.</span>bfloat16<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span><span class=\"token string\">\"cuda:0\"</span><span class=\"token punctuation\">)</span>\nk <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>randn<span class=\"token punctuation\">(</span><span class=\"token number\">16</span><span class=\"token punctuation\">,</span> SEQLEN<span class=\"token punctuation\">,</span> <span class=\"token number\">8</span><span class=\"token punctuation\">,</span> <span class=\"token number\">32</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span>torch<span class=\"token punctuation\">.</span>bfloat16<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span><span class=\"token string\">\"cuda:0\"</span><span class=\"token punctuation\">)</span>\nv <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>randn<span class=\"token punctuation\">(</span><span class=\"token number\">16</span><span class=\"token punctuation\">,</span> SEQLEN<span class=\"token punctuation\">,</span> <span class=\"token number\">8</span><span class=\"token punctuation\">,</span> <span class=\"token number\">32</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span>torch<span class=\"token punctuation\">.</span>bfloat16<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span><span class=\"token string\">\"cuda:0\"</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token operator\">%</span>timeit z <span class=\"token operator\">=</span> modified_forward_varlen<span class=\"token punctuation\">(</span>q<span class=\"token punctuation\">,</span>k<span class=\"token punctuation\">,</span>v<span class=\"token punctuation\">)</span>\n<span class=\"token operator\">%</span>timeit z <span class=\"token operator\">=</span> modified_forward_default<span class=\"token punctuation\">(</span>q<span class=\"token punctuation\">,</span>k<span class=\"token punctuation\">,</span>v<span class=\"token punctuation\">)</span></code></pre>\n<p>For various values of <code>SEQLEN</code>.</p>\n<p>Then, to compute TFLOPs/s, use the resulting time:</p>\n<pre class=\"language-python\" tabindex=\"0\"><code class=\"language-python\"><span class=\"token keyword\">import</span> math\ntime_in_sec <span class=\"token operator\">=</span> <span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span>\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">flops</span><span class=\"token punctuation\">(</span>batch<span class=\"token punctuation\">,</span> seqlen<span class=\"token punctuation\">,</span> headdim<span class=\"token punctuation\">,</span> nheads<span class=\"token punctuation\">,</span> causal<span class=\"token punctuation\">,</span> mode<span class=\"token operator\">=</span><span class=\"token string\">\"fwd\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">assert</span> mode <span class=\"token keyword\">in</span> <span class=\"token punctuation\">[</span><span class=\"token string\">\"fwd\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"bwd\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"fwd_bwd\"</span><span class=\"token punctuation\">]</span>\n    f <span class=\"token operator\">=</span> <span class=\"token number\">4</span> <span class=\"token operator\">*</span> batch <span class=\"token operator\">*</span> seqlen<span class=\"token operator\">**</span><span class=\"token number\">2</span> <span class=\"token operator\">*</span> nheads <span class=\"token operator\">*</span> headdim <span class=\"token operator\">//</span> <span class=\"token punctuation\">(</span><span class=\"token number\">2</span> <span class=\"token keyword\">if</span> causal <span class=\"token keyword\">else</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">return</span> f <span class=\"token keyword\">if</span> mode <span class=\"token operator\">==</span> <span class=\"token string\">\"fwd\"</span> <span class=\"token keyword\">else</span> <span class=\"token punctuation\">(</span><span class=\"token number\">2.5</span> <span class=\"token operator\">*</span> f <span class=\"token keyword\">if</span> mode <span class=\"token operator\">==</span> <span class=\"token string\">\"bwd\"</span> <span class=\"token keyword\">else</span> <span class=\"token number\">3.5</span> <span class=\"token operator\">*</span> f<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">efficiency</span><span class=\"token punctuation\">(</span>flop<span class=\"token punctuation\">,</span> time<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">return</span> <span class=\"token punctuation\">(</span>flop <span class=\"token operator\">/</span> time <span class=\"token operator\">/</span> <span class=\"token number\">10</span><span class=\"token operator\">**</span><span class=\"token number\">12</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">if</span> <span class=\"token keyword\">not</span> math<span class=\"token punctuation\">.</span>isnan<span class=\"token punctuation\">(</span>time<span class=\"token punctuation\">)</span> <span class=\"token keyword\">else</span> <span class=\"token number\">0.0</span>\n\ntflops_s <span class=\"token operator\">=</span> efficiency<span class=\"token punctuation\">(</span>flops<span class=\"token punctuation\">(</span><span class=\"token number\">16</span><span class=\"token punctuation\">,</span> SEQLEN<span class=\"token punctuation\">,</span> <span class=\"token number\">8</span><span class=\"token punctuation\">,</span> <span class=\"token number\">32</span><span class=\"token punctuation\">,</span> <span class=\"token boolean\">False</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> time_in_sec<span class=\"token punctuation\">)</span></code></pre>\n<p>Note that <code>flash_attn_varlen_func</code> defaults to the the default <code>flash_attn_func</code> if given fixed-length sequences.\nFor this reason, this benchmark periodically assign 1 tokens more or less from and to each sequence to force the usage of <code>flash_attn_varlen_func</code>. <a href=\"https://gdewael.github.io/blog/flashattnvarlen/\" class=\"footnote-backref\">↩︎</a></p>\n</li>\n<li id=\"fn3\" class=\"footnote-item\"><p>Implementation of distributed bucket batch sampler:</p>\n<pre class=\"language-python\" tabindex=\"0\"><code class=\"language-python\"><span class=\"token keyword\">class</span> <span class=\"token class-name\">DistributedBucketSampler</span><span class=\"token punctuation\">(</span>DistributedSampler<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__init__</span><span class=\"token punctuation\">(</span>\n        self<span class=\"token punctuation\">,</span>\n        dataset<span class=\"token punctuation\">,</span>\n        batch_size<span class=\"token punctuation\">,</span>\n        n_partitions <span class=\"token operator\">=</span> <span class=\"token number\">100</span><span class=\"token punctuation\">,</span>\n        num_replicas<span class=\"token operator\">=</span><span class=\"token boolean\">None</span><span class=\"token punctuation\">,</span>\n        rank<span class=\"token operator\">=</span><span class=\"token boolean\">None</span><span class=\"token punctuation\">,</span>\n        shuffle<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">,</span>\n        seed<span class=\"token operator\">=</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span>\n        drop_last<span class=\"token operator\">=</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">,</span>\n    <span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token builtin\">super</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>__init__<span class=\"token punctuation\">(</span>\n            dataset<span class=\"token punctuation\">,</span>\n            num_replicas<span class=\"token operator\">=</span>num_replicas<span class=\"token punctuation\">,</span>\n            rank<span class=\"token operator\">=</span>rank<span class=\"token punctuation\">,</span>\n            shuffle<span class=\"token operator\">=</span>shuffle<span class=\"token punctuation\">,</span>\n            seed<span class=\"token operator\">=</span>seed<span class=\"token punctuation\">,</span>\n            drop_last<span class=\"token operator\">=</span>drop_last\n        <span class=\"token punctuation\">)</span>\n\n        self<span class=\"token punctuation\">.</span>batch_size <span class=\"token operator\">=</span> batch_size\n        self<span class=\"token punctuation\">.</span>n_partitions <span class=\"token operator\">=</span> n_partitions\n\n        self<span class=\"token punctuation\">.</span>_len <span class=\"token operator\">=</span> _partitions_to_len<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>num_samples<span class=\"token punctuation\">,</span> n_partitions<span class=\"token punctuation\">,</span> batch_size<span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__iter__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token comment\"># Inherit a list of indices from parent class DistributedSampler</span>\n        indices <span class=\"token operator\">=</span> <span class=\"token builtin\">list</span><span class=\"token punctuation\">(</span><span class=\"token builtin\">super</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>__iter__<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n        <span class=\"token comment\"># Use it to create a bucketbatchSampler</span>\n        batch_sampler <span class=\"token operator\">=</span> BucketBatchSampler<span class=\"token punctuation\">(</span>\n            self<span class=\"token punctuation\">.</span>dataset<span class=\"token punctuation\">,</span>\n            batch_size<span class=\"token operator\">=</span>self<span class=\"token punctuation\">.</span>batch_size<span class=\"token punctuation\">,</span>\n            n_partitions<span class=\"token operator\">=</span>self<span class=\"token punctuation\">.</span>n_partitions<span class=\"token punctuation\">,</span>\n            indices <span class=\"token operator\">=</span> indices\n            <span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">return</span> <span class=\"token builtin\">iter</span><span class=\"token punctuation\">(</span>batch_sampler<span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__len__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">return</span> self<span class=\"token punctuation\">.</span>_len</code></pre>\n<p>Note that if you're using PyTorch-Lightning, note that <code>Trainer</code> would automatically instate its own sampler if using the <code>ddp</code> strategy, hence overriding this self-defined sampler.\nTo prevent this behavior, make sure to use <code>use_distributed_sampler=False</code> when calling <code>Trainer</code>. <a href=\"https://gdewael.github.io/blog/flashattnvarlen/\" class=\"footnote-backref\">↩︎</a></p>\n</li>\n<li id=\"fn4\" class=\"footnote-item\"><p>Cui, Haotian, et al. &quot;scGPT: toward building a foundation model for single-cell multi-omics using generative AI.&quot; Nature Methods (2024): 1-11. <a href=\"https://gdewael.github.io/blog/flashattnvarlen/\" class=\"footnote-backref\">↩︎</a></p>\n</li>\n</ol>\n</section>\n",
			"date_published": "2024-08-21T00:00:00Z"
		}
		,
		{
			"id": "https://gdewael.github.io/blog/equivariance/",
			"url": "https://gdewael.github.io/blog/equivariance/",
			"title": "Visualizing equivariances in transformer neural networks",
			"content_html": "<p>Transformers neural networks have become the dominant architecture within many subfields of deep learning.\nTheir success is partly owed due to the fact that self-attention is a very generic operation in terms of the geometric priors it uses<sup class=\"footnote-ref\"><a href=\"https://gdewael.github.io/blog/equivariance/\" id=\"fnref1\">[1]</a></sup>.\nThe following blogpost interactively visualizes some geometric priors in transformers.\nThe target audience for this blogpost are those who are already (at least vaguely) familiar with self-attention and want to see some simple visualizations of what positional encodings do to them<sup class=\"footnote-ref\"><a href=\"https://gdewael.github.io/blog/equivariance/\" id=\"fnref2\">[2]</a></sup>.</p>\n<p>As a quick prerequisite, let us recap the self-attention formula via a simple example.\nConsider the sentence: &quot;Love conquers all&quot;.\nEach word in this sentence can be assigned an embedding vector, which may look like the following:</p>\n<div style=\"text-align: center;\" id=\"vis-x\"></div>\n<p>These three vectors gives us an input matrix <mjx-container class=\"MathJax\" jax=\"SVG\" style=\"direction: ltr; position: relative;\"><svg style=\"overflow: visible; min-height: 1px; min-width: 1px; vertical-align: -0.09ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"9.766ex\" height=\"2.022ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -853.7 4316.6 893.7\" aria-hidden=\"true\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"mi\"><path data-c=\"1D47F\" d=\"M931 686Q953 686 953 670Q953 650 944 632Q936 624 924 624H914Q823 624 803 611Q800 609 696 503T591 396Q591 394 667 229L743 62H787H814Q846 62 846 44Q843 7 829 2Q825 0 817 0Q813 0 775 1T664 2Q590 2 551 1T508 0H507Q484 0 484 18Q484 19 488 37Q492 56 497 58T534 62L566 63Q567 64 520 169T471 274Q469 274 369 172T268 67L315 62Q320 62 328 62L335 61Q347 58 347 44Q344 10 331 2L326 0L287 1Q263 2 177 2Q95 2 78 1L53 0Q38 6 38 17Q38 40 50 57Q56 62 78 62Q169 62 188 75Q194 77 435 324L444 334L439 347Q437 351 373 492L313 624H268H246Q220 624 212 632Q210 636 210 642Q210 655 215 669T227 684Q230 686 247 686Q295 684 398 684Q438 684 472 684T527 685T551 686Q567 686 572 671Q572 667 568 651Q563 631 558 628T523 624T492 623H488L526 540Q563 457 564 457Q564 456 574 466T604 496T645 537L724 619Q716 622 677 624H673Q645 624 645 640Q645 660 654 678Q659 683 666 686L704 685Q728 684 813 684Q847 684 873 684T913 685T931 686Z\" style=\"stroke-width: 3;\"></path></g><g data-mml-node=\"mo\" transform=\"translate(1224.8,0)\"><path data-c=\"2208\" d=\"M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z\" style=\"stroke-width: 3;\"></path></g><g data-mml-node=\"msup\" transform=\"translate(2169.6,0)\"><g data-mml-node=\"TeXAtom\" data-mjx-texclass=\"ORD\"><g data-mml-node=\"mi\"><path data-c=\"211D\" d=\"M17 665Q17 672 28 683H221Q415 681 439 677Q461 673 481 667T516 654T544 639T566 623T584 607T597 592T607 578T614 565T618 554L621 548Q626 530 626 497Q626 447 613 419Q578 348 473 326L455 321Q462 310 473 292T517 226T578 141T637 72T686 35Q705 30 705 16Q705 7 693 -1H510Q503 6 404 159L306 310H268V183Q270 67 271 59Q274 42 291 38Q295 37 319 35Q344 35 353 28Q362 17 353 3L346 -1H28Q16 5 16 16Q16 35 55 35Q96 38 101 52Q106 60 106 341T101 632Q95 645 55 648Q17 648 17 665ZM241 35Q238 42 237 45T235 78T233 163T233 337V621L237 635L244 648H133Q136 641 137 638T139 603T141 517T141 341Q141 131 140 89T134 37Q133 36 133 35H241ZM457 496Q457 540 449 570T425 615T400 634T377 643Q374 643 339 648Q300 648 281 635Q271 628 270 610T268 481V346H284Q327 346 375 352Q421 364 439 392T457 496ZM492 537T492 496T488 427T478 389T469 371T464 361Q464 360 465 360Q469 360 497 370Q593 400 593 495Q593 592 477 630L457 637L461 626Q474 611 488 561Q492 537 492 496ZM464 243Q411 317 410 317Q404 317 401 315Q384 315 370 312H346L526 35H619L606 50Q553 109 464 243Z\" style=\"stroke-width: 3;\"></path></g></g><g data-mml-node=\"TeXAtom\" transform=\"translate(755,363) scale(0.707)\" data-mjx-texclass=\"ORD\"><g data-mml-node=\"mi\"><path data-c=\"1D45B\" d=\"M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z\" style=\"stroke-width: 3;\"></path></g><g data-mml-node=\"mo\" transform=\"translate(600,0)\"><path data-c=\"D7\" d=\"M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z\" style=\"stroke-width: 3;\"></path></g><g data-mml-node=\"mi\" transform=\"translate(1378,0)\"><path data-c=\"1D451\" d=\"M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z\" style=\"stroke-width: 3;\"></path></g></g></g></g></g></svg><mjx-assistive-mml unselectable=\"on\" display=\"inline\" style=\"top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi mathvariant=\"bold-italic\">X</mi><mo>∈</mo><msup><mrow data-mjx-texclass=\"ORD\"><mi mathvariant=\"double-struck\">R</mi></mrow><mrow data-mjx-texclass=\"ORD\"><mi>n</mi><mo>×</mo><mi>d</mi></mrow></msup></math></mjx-assistive-mml></mjx-container> consisting of <mjx-container class=\"MathJax\" jax=\"SVG\" style=\"direction: ltr; position: relative;\"><svg style=\"overflow: visible; min-height: 1px; min-width: 1px; vertical-align: -0.186ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"5.506ex\" height=\"1.69ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -665 2433.6 747\" aria-hidden=\"true\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"mi\"><path data-c=\"1D45B\" d=\"M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z\" style=\"stroke-width: 3;\"></path></g><g data-mml-node=\"mo\" transform=\"translate(877.8,0)\"><path data-c=\"3D\" d=\"M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z\" style=\"stroke-width: 3;\"></path></g><g data-mml-node=\"mn\" transform=\"translate(1933.6,0)\"><path data-c=\"33\" d=\"M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z\" style=\"stroke-width: 3;\"></path></g></g></g></svg><mjx-assistive-mml unselectable=\"on\" display=\"inline\" style=\"top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>n</mi><mo>=</mo><mn>3</mn></math></mjx-assistive-mml></mjx-container> input elements, each with <mjx-container class=\"MathJax\" jax=\"SVG\" style=\"direction: ltr; position: relative;\"><svg style=\"overflow: visible; min-height: 1px; min-width: 1px; vertical-align: -0.186ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"5.325ex\" height=\"1.756ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -694 2353.6 776\" aria-hidden=\"true\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"mi\"><path data-c=\"1D451\" d=\"M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z\" style=\"stroke-width: 3;\"></path></g><g data-mml-node=\"mo\" transform=\"translate(797.8,0)\"><path data-c=\"3D\" d=\"M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z\" style=\"stroke-width: 3;\"></path></g><g data-mml-node=\"mn\" transform=\"translate(1853.6,0)\"><path data-c=\"32\" d=\"M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z\" style=\"stroke-width: 3;\"></path></g></g></g></svg><mjx-assistive-mml unselectable=\"on\" display=\"inline\" style=\"top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>d</mi><mo>=</mo><mn>2</mn></math></mjx-assistive-mml></mjx-container> (hidden) features.\nTo perform self-attention on this input matrix <mjx-container class=\"MathJax\" jax=\"SVG\" style=\"direction: ltr; position: relative;\"><svg style=\"overflow: visible; min-height: 1px; min-width: 1px; vertical-align: 0;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"2.143ex\" height=\"1.552ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -686 947 686\" aria-hidden=\"true\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"mi\"><path data-c=\"1D47F\" d=\"M931 686Q953 686 953 670Q953 650 944 632Q936 624 924 624H914Q823 624 803 611Q800 609 696 503T591 396Q591 394 667 229L743 62H787H814Q846 62 846 44Q843 7 829 2Q825 0 817 0Q813 0 775 1T664 2Q590 2 551 1T508 0H507Q484 0 484 18Q484 19 488 37Q492 56 497 58T534 62L566 63Q567 64 520 169T471 274Q469 274 369 172T268 67L315 62Q320 62 328 62L335 61Q347 58 347 44Q344 10 331 2L326 0L287 1Q263 2 177 2Q95 2 78 1L53 0Q38 6 38 17Q38 40 50 57Q56 62 78 62Q169 62 188 75Q194 77 435 324L444 334L439 347Q437 351 373 492L313 624H268H246Q220 624 212 632Q210 636 210 642Q210 655 215 669T227 684Q230 686 247 686Q295 684 398 684Q438 684 472 684T527 685T551 686Q567 686 572 671Q572 667 568 651Q563 631 558 628T523 624T492 623H488L526 540Q563 457 564 457Q564 456 574 466T604 496T645 537L724 619Q716 622 677 624H673Q645 624 645 640Q645 660 654 678Q659 683 666 686L704 685Q728 684 813 684Q847 684 873 684T913 685T931 686Z\" style=\"stroke-width: 3;\"></path></g></g></g></svg><mjx-assistive-mml unselectable=\"on\" display=\"inline\" style=\"top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi mathvariant=\"bold-italic\">X</mi></math></mjx-assistive-mml></mjx-container>, one first takes three linear transformations of <mjx-container class=\"MathJax\" jax=\"SVG\" style=\"direction: ltr; position: relative;\"><svg style=\"overflow: visible; min-height: 1px; min-width: 1px; vertical-align: 0;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"2.143ex\" height=\"1.552ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -686 947 686\" aria-hidden=\"true\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"mi\"><path data-c=\"1D47F\" d=\"M931 686Q953 686 953 670Q953 650 944 632Q936 624 924 624H914Q823 624 803 611Q800 609 696 503T591 396Q591 394 667 229L743 62H787H814Q846 62 846 44Q843 7 829 2Q825 0 817 0Q813 0 775 1T664 2Q590 2 551 1T508 0H507Q484 0 484 18Q484 19 488 37Q492 56 497 58T534 62L566 63Q567 64 520 169T471 274Q469 274 369 172T268 67L315 62Q320 62 328 62L335 61Q347 58 347 44Q344 10 331 2L326 0L287 1Q263 2 177 2Q95 2 78 1L53 0Q38 6 38 17Q38 40 50 57Q56 62 78 62Q169 62 188 75Q194 77 435 324L444 334L439 347Q437 351 373 492L313 624H268H246Q220 624 212 632Q210 636 210 642Q210 655 215 669T227 684Q230 686 247 686Q295 684 398 684Q438 684 472 684T527 685T551 686Q567 686 572 671Q572 667 568 651Q563 631 558 628T523 624T492 623H488L526 540Q563 457 564 457Q564 456 574 466T604 496T645 537L724 619Q716 622 677 624H673Q645 624 645 640Q645 660 654 678Q659 683 666 686L704 685Q728 684 813 684Q847 684 873 684T913 685T931 686Z\" style=\"stroke-width: 3;\"></path></g></g></g></svg><mjx-assistive-mml unselectable=\"on\" display=\"inline\" style=\"top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi mathvariant=\"bold-italic\">X</mi></math></mjx-assistive-mml></mjx-container> with learned weights <mjx-container class=\"MathJax\" jax=\"SVG\" style=\"direction: ltr; position: relative;\"><svg style=\"overflow: visible; min-height: 1px; min-width: 1px; vertical-align: -0.8ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"16.61ex\" height=\"2.731ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -853.7 7341.5 1207.2\" aria-hidden=\"true\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"msub\"><g data-mml-node=\"mi\"><path data-c=\"1D47E\" d=\"M111 624Q109 624 102 624T91 623Q61 623 61 640Q61 660 70 678Q78 686 98 686Q140 684 239 684Q277 684 309 684T360 685T383 686H385Q407 686 407 668Q404 634 391 626Q387 624 348 624Q307 624 307 622Q307 618 332 409Q359 198 359 195L570 532L564 576L558 622V624H522H504Q472 624 472 641Q475 678 488 684L493 686L529 685Q551 684 645 684Q716 684 753 685T795 686Q818 686 818 669Q815 632 802 626Q798 624 759 624Q718 624 718 622Q718 615 743 410Q770 199 770 196Q770 195 806 253T903 406Q1035 618 1035 619Q1025 624 968 624Q943 624 943 641Q943 648 946 659Q950 675 952 679T963 686L998 685Q1020 684 1093 684Q1113 684 1139 685T1173 686Q1207 686 1207 669Q1207 664 1204 652Q1199 631 1194 628T1164 624Q1113 622 1101 615Q1098 612 905 305Q715 -1 709 -7Q699 -17 673 -17Q645 -17 639 -8L581 441Q581 444 442 221Q331 44 314 18T288 -14Q279 -17 263 -17H254Q229 -17 227 -5Q225 2 186 311L147 620V624H111Z\" style=\"stroke-width: 3;\"></path></g><g data-mml-node=\"TeXAtom\" transform=\"translate(1126,-176.7) scale(0.707)\" data-mjx-texclass=\"ORD\"><g data-mml-node=\"mi\"><path data-c=\"1D456\" d=\"M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z\" style=\"stroke-width: 3;\"></path></g><g data-mml-node=\"mo\" transform=\"translate(345,0)\"><path data-c=\"2208\" d=\"M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z\" style=\"stroke-width: 3;\"></path></g><g data-mml-node=\"mo\" transform=\"translate(1012,0)\"><path data-c=\"7B\" d=\"M434 -231Q434 -244 428 -250H410Q281 -250 230 -184Q225 -177 222 -172T217 -161T213 -148T211 -133T210 -111T209 -84T209 -47T209 0Q209 21 209 53Q208 142 204 153Q203 154 203 155Q189 191 153 211T82 231Q71 231 68 234T65 250T68 266T82 269Q116 269 152 289T203 345Q208 356 208 377T209 529V579Q209 634 215 656T244 698Q270 724 324 740Q361 748 377 749Q379 749 390 749T408 750H428Q434 744 434 732Q434 719 431 716Q429 713 415 713Q362 710 332 689T296 647Q291 634 291 499V417Q291 370 288 353T271 314Q240 271 184 255L170 250L184 245Q202 239 220 230T262 196T290 137Q291 131 291 1Q291 -134 296 -147Q306 -174 339 -192T415 -213Q429 -213 431 -216Q434 -219 434 -231Z\" style=\"stroke-width: 3;\"></path></g><g data-mml-node=\"mi\" transform=\"translate(1512,0)\"><path data-c=\"1D45E\" d=\"M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z\" style=\"stroke-width: 3;\"></path></g><g data-mml-node=\"mo\" transform=\"translate(1972,0)\"><path data-c=\"2C\" d=\"M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z\" style=\"stroke-width: 3;\"></path></g><g data-mml-node=\"mi\" transform=\"translate(2250,0)\"><path data-c=\"1D458\" d=\"M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z\" style=\"stroke-width: 3;\"></path></g><g data-mml-node=\"mo\" transform=\"translate(2771,0)\"><path data-c=\"2C\" d=\"M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z\" style=\"stroke-width: 3;\"></path></g><g data-mml-node=\"mi\" transform=\"translate(3049,0)\"><path data-c=\"1D463\" d=\"M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z\" style=\"stroke-width: 3;\"></path></g><g data-mml-node=\"mo\" transform=\"translate(3534,0)\"><path data-c=\"7D\" d=\"M65 731Q65 745 68 747T88 750Q171 750 216 725T279 670Q288 649 289 635T291 501Q292 362 293 357Q306 312 345 291T417 269Q428 269 431 266T434 250T431 234T417 231Q380 231 345 210T298 157Q293 143 292 121T291 -28V-79Q291 -134 285 -156T256 -198Q202 -250 89 -250Q71 -250 68 -247T65 -230Q65 -224 65 -223T66 -218T69 -214T77 -213Q91 -213 108 -210T146 -200T183 -177T207 -139Q208 -134 209 3L210 139Q223 196 280 230Q315 247 330 250Q305 257 280 270Q225 304 212 352L210 362L209 498Q208 635 207 640Q195 680 154 696T77 713Q68 713 67 716T65 731Z\" style=\"stroke-width: 3;\"></path></g></g></g><g data-mml-node=\"mo\" transform=\"translate(4306.2,0)\"><path data-c=\"2208\" d=\"M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z\" style=\"stroke-width: 3;\"></path></g><g data-mml-node=\"msup\" transform=\"translate(5251,0)\"><g data-mml-node=\"TeXAtom\" data-mjx-texclass=\"ORD\"><g data-mml-node=\"mi\"><path data-c=\"211D\" d=\"M17 665Q17 672 28 683H221Q415 681 439 677Q461 673 481 667T516 654T544 639T566 623T584 607T597 592T607 578T614 565T618 554L621 548Q626 530 626 497Q626 447 613 419Q578 348 473 326L455 321Q462 310 473 292T517 226T578 141T637 72T686 35Q705 30 705 16Q705 7 693 -1H510Q503 6 404 159L306 310H268V183Q270 67 271 59Q274 42 291 38Q295 37 319 35Q344 35 353 28Q362 17 353 3L346 -1H28Q16 5 16 16Q16 35 55 35Q96 38 101 52Q106 60 106 341T101 632Q95 645 55 648Q17 648 17 665ZM241 35Q238 42 237 45T235 78T233 163T233 337V621L237 635L244 648H133Q136 641 137 638T139 603T141 517T141 341Q141 131 140 89T134 37Q133 36 133 35H241ZM457 496Q457 540 449 570T425 615T400 634T377 643Q374 643 339 648Q300 648 281 635Q271 628 270 610T268 481V346H284Q327 346 375 352Q421 364 439 392T457 496ZM492 537T492 496T488 427T478 389T469 371T464 361Q464 360 465 360Q469 360 497 370Q593 400 593 495Q593 592 477 630L457 637L461 626Q474 611 488 561Q492 537 492 496ZM464 243Q411 317 410 317Q404 317 401 315Q384 315 370 312H346L526 35H619L606 50Q553 109 464 243Z\" style=\"stroke-width: 3;\"></path></g></g><g data-mml-node=\"TeXAtom\" transform=\"translate(755,363) scale(0.707)\" data-mjx-texclass=\"ORD\"><g data-mml-node=\"mi\"><path data-c=\"1D451\" d=\"M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z\" style=\"stroke-width: 3;\"></path></g><g data-mml-node=\"mo\" transform=\"translate(520,0)\"><path data-c=\"D7\" d=\"M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z\" style=\"stroke-width: 3;\"></path></g><g data-mml-node=\"mi\" transform=\"translate(1298,0)\"><path data-c=\"1D451\" d=\"M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z\" style=\"stroke-width: 3;\"></path></g></g></g></g></g></svg><mjx-assistive-mml unselectable=\"on\" display=\"inline\" style=\"top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi mathvariant=\"bold-italic\">W</mi><mrow data-mjx-texclass=\"ORD\"><mi>i</mi><mo>∈</mo><mo fence=\"false\" stretchy=\"false\">{</mo><mi>q</mi><mo>,</mo><mi>k</mi><mo>,</mo><mi>v</mi><mo fence=\"false\" stretchy=\"false\">}</mo></mrow></msub><mo>∈</mo><msup><mrow data-mjx-texclass=\"ORD\"><mi mathvariant=\"double-struck\">R</mi></mrow><mrow data-mjx-texclass=\"ORD\"><mi>d</mi><mo>×</mo><mi>d</mi></mrow></msup></math></mjx-assistive-mml></mjx-container>. Visualized:</p>\n<div style=\"text-align: center;\" id=\"vis-qkv\"></div>\n<p>Note that I have chosen simple word embeddings and projections to simplify following along with computations.\nIn practice, these weights are learned.</p>\n<p>After projection, self-attention is performed as follows:</p>\n<mjx-container class=\"MathJax\" jax=\"SVG\" display=\"true\" style=\"direction: ltr; display: block; text-align: center; margin: 1em 0; position: relative;\"><svg style=\"overflow: visible; min-height: 1px; min-width: 1px; vertical-align: -2.308ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"25.136ex\" height=\"5.727ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -1511.3 11110 2531.3\" aria-hidden=\"true\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"mi\"><path data-c=\"1D481\" d=\"M223 430Q192 430 192 448Q192 450 225 561T261 677Q265 683 270 684Q273 686 534 686Q796 686 797 685Q805 682 805 673Q805 668 804 661T800 648T798 641Q796 637 531 352L266 67L329 66H364Q412 66 446 70T523 96T596 157Q617 186 630 220T649 273T663 297Q667 299 684 299H688Q715 299 715 281Q715 278 673 145T628 8Q626 4 617 0H348Q289 0 221 0T139 -1Q112 -1 99 -1T78 1T69 5T68 12Q68 16 71 31T77 49L84 57Q91 65 104 79T133 110T170 151T213 196L610 624H540Q533 624 514 624T488 624T467 623T443 620T422 616T398 609T373 600Q292 560 255 449Q251 436 246 433T223 430Z\" style=\"stroke-width: 3;\"></path></g><g data-mml-node=\"mo\" transform=\"translate(1082.8,0)\"><path data-c=\"3D\" d=\"M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z\" style=\"stroke-width: 3;\"></path></g><g data-mml-node=\"TeXAtom\" data-mjx-texclass=\"ORD\" transform=\"translate(2138.6,0)\"><g data-mml-node=\"mi\"><path data-c=\"73\" d=\"M295 316Q295 356 268 385T190 414Q154 414 128 401Q98 382 98 349Q97 344 98 336T114 312T157 287Q175 282 201 278T245 269T277 256Q294 248 310 236T342 195T359 133Q359 71 321 31T198 -10H190Q138 -10 94 26L86 19L77 10Q71 4 65 -1L54 -11H46H42Q39 -11 33 -5V74V132Q33 153 35 157T45 162H54Q66 162 70 158T75 146T82 119T101 77Q136 26 198 26Q295 26 295 104Q295 133 277 151Q257 175 194 187T111 210Q75 227 54 256T33 318Q33 357 50 384T93 424T143 442T187 447H198Q238 447 268 432L283 424L292 431Q302 440 314 448H322H326Q329 448 335 442V310L329 304H301Q295 310 295 316Z\" style=\"stroke-width: 3;\"></path><path data-c=\"6F\" d=\"M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z\" transform=\"translate(394,0)\" style=\"stroke-width: 3;\"></path><path data-c=\"66\" d=\"M273 0Q255 3 146 3Q43 3 34 0H26V46H42Q70 46 91 49Q99 52 103 60Q104 62 104 224V385H33V431H104V497L105 564L107 574Q126 639 171 668T266 704Q267 704 275 704T289 705Q330 702 351 679T372 627Q372 604 358 590T321 576T284 590T270 627Q270 647 288 667H284Q280 668 273 668Q245 668 223 647T189 592Q183 572 182 497V431H293V385H185V225Q185 63 186 61T189 57T194 54T199 51T206 49T213 48T222 47T231 47T241 46T251 46H282V0H273Z\" transform=\"translate(894,0)\" style=\"stroke-width: 3;\"></path><path data-c=\"74\" d=\"M27 422Q80 426 109 478T141 600V615H181V431H316V385H181V241Q182 116 182 100T189 68Q203 29 238 29Q282 29 292 100Q293 108 293 146V181H333V146V134Q333 57 291 17Q264 -10 221 -10Q187 -10 162 2T124 33T105 68T98 100Q97 107 97 248V385H18V422H27Z\" transform=\"translate(1200,0)\" style=\"stroke-width: 3;\"></path><path data-c=\"6D\" d=\"M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z\" transform=\"translate(1589,0)\" style=\"stroke-width: 3;\"></path><path data-c=\"61\" d=\"M137 305T115 305T78 320T63 359Q63 394 97 421T218 448Q291 448 336 416T396 340Q401 326 401 309T402 194V124Q402 76 407 58T428 40Q443 40 448 56T453 109V145H493V106Q492 66 490 59Q481 29 455 12T400 -6T353 12T329 54V58L327 55Q325 52 322 49T314 40T302 29T287 17T269 6T247 -2T221 -8T190 -11Q130 -11 82 20T34 107Q34 128 41 147T68 188T116 225T194 253T304 268H318V290Q318 324 312 340Q290 411 215 411Q197 411 181 410T156 406T148 403Q170 388 170 359Q170 334 154 320ZM126 106Q126 75 150 51T209 26Q247 26 276 49T315 109Q317 116 318 175Q318 233 317 233Q309 233 296 232T251 223T193 203T147 166T126 106Z\" transform=\"translate(2422,0)\" style=\"stroke-width: 3;\"></path><path data-c=\"78\" d=\"M201 0Q189 3 102 3Q26 3 17 0H11V46H25Q48 47 67 52T96 61T121 78T139 96T160 122T180 150L226 210L168 288Q159 301 149 315T133 336T122 351T113 363T107 370T100 376T94 379T88 381T80 383Q74 383 44 385H16V431H23Q59 429 126 429Q219 429 229 431H237V385Q201 381 201 369Q201 367 211 353T239 315T268 274L272 270L297 304Q329 345 329 358Q329 364 327 369T322 376T317 380T310 384L307 385H302V431H309Q324 428 408 428Q487 428 493 431H499V385H492Q443 385 411 368Q394 360 377 341T312 257L296 236L358 151Q424 61 429 57T446 50Q464 46 499 46H516V0H510H502Q494 1 482 1T457 2T432 2T414 3Q403 3 377 3T327 1L304 0H295V46H298Q309 46 320 51T331 63Q331 65 291 120L250 175Q249 174 219 133T185 88Q181 83 181 74Q181 63 188 55T206 46Q208 46 208 23V0H201Z\" transform=\"translate(2922,0)\" style=\"stroke-width: 3;\"></path></g></g><g data-mml-node=\"mrow\" transform=\"translate(5755.2,0)\"><g data-mml-node=\"mo\" transform=\"translate(0 -0.5)\"><path data-c=\"28\" d=\"M701 -940Q701 -943 695 -949H664Q662 -947 636 -922T591 -879T537 -818T475 -737T412 -636T350 -511T295 -362T250 -186T221 17T209 251Q209 962 573 1361Q596 1386 616 1405T649 1437T664 1450H695Q701 1444 701 1441Q701 1436 681 1415T629 1356T557 1261T476 1118T400 927T340 675T308 359Q306 321 306 250Q306 -139 400 -430T690 -924Q701 -936 701 -940Z\" style=\"stroke-width: 3;\"></path></g><g data-mml-node=\"mfrac\" transform=\"translate(736,0)\"><g data-mml-node=\"mrow\" transform=\"translate(220,676)\"><g data-mml-node=\"mi\"><path data-c=\"1D478\" d=\"M53 245Q53 297 70 356T125 478T216 590T349 671T523 703Q656 703 735 637T815 445Q815 410 808 370T781 277T729 178T643 87T519 14L525 4Q540 -19 553 -25T592 -32Q632 -32 654 -24T680 -7T689 10T704 18Q713 18 717 12T722 0Q722 -8 711 -36T681 -101T624 -166T541 -194Q513 -194 494 -183T465 -157T450 -118T444 -79T443 -41V-7L433 -9Q391 -17 344 -17Q301 -17 263 -10T185 15T118 62T71 138T53 245ZM666 482Q666 529 652 563T614 615T565 640T512 648Q412 648 335 573Q268 506 235 389T201 202Q201 164 210 136T230 95T259 66L262 76Q269 109 302 135T382 162Q401 162 415 159T449 140T484 92L491 78L496 82Q502 86 505 88T515 97T528 107T541 120T555 137T570 156T585 179T599 205T612 235Q629 278 647 351T666 482ZM439 56Q439 58 439 62T435 75T426 92T410 106T383 112Q353 112 332 96T311 63Q311 38 355 38H366Q391 39 415 45T439 56Z\" style=\"stroke-width: 3;\"></path></g><g data-mml-node=\"msup\" transform=\"translate(869,0)\"><g data-mml-node=\"mi\"><path data-c=\"1D472\" d=\"M536 0Q522 6 522 18Q522 35 533 57Q539 62 557 62Q595 62 601 65L472 330L365 255L342 160Q318 65 317 64Q317 62 361 62H388Q420 62 420 44Q416 6 403 2L399 0L357 1Q331 2 224 2Q149 2 109 1T65 0Q48 0 43 15Q47 54 60 60Q64 62 113 62H162L302 623Q302 624 258 624H235Q214 624 209 626T199 639Q203 678 216 684Q220 686 239 686Q290 684 403 684Q475 684 512 685T553 686Q576 686 576 668Q572 632 560 626Q555 624 506 624H457L422 481Q386 339 386 337L785 621Q779 624 749 624Q726 624 726 641Q726 645 730 659Q734 675 736 679T747 686L786 685Q812 684 888 684Q908 684 934 685T968 686Q1003 686 1003 669Q1003 646 991 629Q985 624 967 624Q918 624 888 617Q884 617 874 613L865 609Q864 608 732 515T599 420Q599 418 686 242T775 65Q784 62 829 62Q847 62 850 61T860 54Q862 52 862 43Q862 10 845 1Q844 1 842 1T836 0T797 1T694 2Q599 2 573 1L536 0Z\" style=\"stroke-width: 3;\"></path></g><g data-mml-node=\"mi\" transform=\"translate(1087.6,363) scale(0.707)\"><path data-c=\"22A4\" d=\"M55 642T55 648T59 659T66 666T71 668H708Q723 660 723 648T708 628H409V15Q402 2 391 0Q387 0 384 1T379 3T375 6T373 9T371 13T369 16V628H71Q70 628 67 630T59 637Z\" style=\"stroke-width: 3;\"></path></g></g></g><g data-mml-node=\"msqrt\" transform=\"translate(811.9,-929.5)\"><g transform=\"translate(853,0)\"><g data-mml-node=\"mi\"><path data-c=\"1D451\" d=\"M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z\" style=\"stroke-width: 3;\"></path></g></g><g data-mml-node=\"mo\" transform=\"translate(0,109.5)\"><path data-c=\"221A\" d=\"M95 178Q89 178 81 186T72 200T103 230T169 280T207 309Q209 311 212 311H213Q219 311 227 294T281 177Q300 134 312 108L397 -77Q398 -77 501 136T707 565T814 786Q820 800 834 800Q841 800 846 794T853 782V776L620 293L385 -193Q381 -200 366 -200Q357 -200 354 -197Q352 -195 256 15L160 225L144 214Q129 202 113 190T95 178Z\" style=\"stroke-width: 3;\"></path></g><rect width=\"520\" height=\"60\" x=\"853\" y=\"849.5\"></rect></g><rect width=\"2756.7\" height=\"60\" x=\"120\" y=\"220\"></rect></g><g data-mml-node=\"mo\" transform=\"translate(3732.7,0) translate(0 -0.5)\"><path data-c=\"29\" d=\"M34 1438Q34 1446 37 1448T50 1450H56H71Q73 1448 99 1423T144 1380T198 1319T260 1238T323 1137T385 1013T440 864T485 688T514 485T526 251Q526 134 519 53Q472 -519 162 -860Q139 -885 119 -904T86 -936T71 -949H56Q43 -949 39 -947T34 -937Q88 -883 140 -813Q428 -430 428 251Q428 453 402 628T338 922T245 1146T145 1309T46 1425Q44 1427 42 1429T39 1433T36 1436L34 1438Z\" style=\"stroke-width: 3;\"></path></g></g><g data-mml-node=\"mi\" transform=\"translate(10224,0)\"><path data-c=\"1D47D\" d=\"M401 686Q415 680 415 668Q415 651 404 629Q398 624 356 624Q318 624 318 623Q318 620 337 508T377 284L397 174L472 285Q548 396 623 507T699 620Q698 621 652 624Q634 624 627 627T619 641Q619 648 622 658Q627 677 631 681T650 686Q654 686 686 685T766 684Q794 684 823 684T858 685Q874 685 878 683T886 671Q886 667 882 651Q877 632 873 628T850 624Q800 624 779 617Q774 617 770 613Q767 610 560 304T350 -5Q346 -9 332 -16H306H291Q270 -16 267 -2Q267 -1 260 37T238 161T210 313L156 624H116H94Q62 624 62 642Q66 678 78 684Q82 686 99 686Q144 684 246 684Q330 684 368 685L401 686Z\" style=\"stroke-width: 3;\"></path></g></g></g></svg><mjx-assistive-mml unselectable=\"on\" display=\"block\" style=\"top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; overflow: hidden; width: 100%;\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi mathvariant=\"bold-italic\">Z</mi><mo>=</mo><mrow data-mjx-texclass=\"ORD\"><mi data-mjx-auto-op=\"false\">softmax</mi></mrow><mrow data-mjx-texclass=\"INNER\"><mo data-mjx-texclass=\"OPEN\">(</mo><mfrac><mrow><mi mathvariant=\"bold-italic\">Q</mi><msup><mi mathvariant=\"bold-italic\">K</mi><mi mathvariant=\"normal\">⊤</mi></msup></mrow><msqrt><mi>d</mi></msqrt></mfrac><mo data-mjx-texclass=\"CLOSE\">)</mo></mrow><mi mathvariant=\"bold-italic\">V</mi></math></mjx-assistive-mml></mjx-container><p>With <mjx-container class=\"MathJax\" jax=\"SVG\" style=\"direction: ltr; position: relative;\"><svg style=\"overflow: visible; min-height: 1px; min-width: 1px; vertical-align: -1.469ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"15.97ex\" height=\"4.07ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -1149.5 7058.5 1799\" aria-hidden=\"true\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"TeXAtom\" data-mjx-texclass=\"ORD\"><g data-mml-node=\"mi\"><path data-c=\"73\" d=\"M295 316Q295 356 268 385T190 414Q154 414 128 401Q98 382 98 349Q97 344 98 336T114 312T157 287Q175 282 201 278T245 269T277 256Q294 248 310 236T342 195T359 133Q359 71 321 31T198 -10H190Q138 -10 94 26L86 19L77 10Q71 4 65 -1L54 -11H46H42Q39 -11 33 -5V74V132Q33 153 35 157T45 162H54Q66 162 70 158T75 146T82 119T101 77Q136 26 198 26Q295 26 295 104Q295 133 277 151Q257 175 194 187T111 210Q75 227 54 256T33 318Q33 357 50 384T93 424T143 442T187 447H198Q238 447 268 432L283 424L292 431Q302 440 314 448H322H326Q329 448 335 442V310L329 304H301Q295 310 295 316Z\" style=\"stroke-width: 3;\"></path><path data-c=\"6F\" d=\"M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z\" transform=\"translate(394,0)\" style=\"stroke-width: 3;\"></path><path data-c=\"66\" d=\"M273 0Q255 3 146 3Q43 3 34 0H26V46H42Q70 46 91 49Q99 52 103 60Q104 62 104 224V385H33V431H104V497L105 564L107 574Q126 639 171 668T266 704Q267 704 275 704T289 705Q330 702 351 679T372 627Q372 604 358 590T321 576T284 590T270 627Q270 647 288 667H284Q280 668 273 668Q245 668 223 647T189 592Q183 572 182 497V431H293V385H185V225Q185 63 186 61T189 57T194 54T199 51T206 49T213 48T222 47T231 47T241 46T251 46H282V0H273Z\" transform=\"translate(894,0)\" style=\"stroke-width: 3;\"></path><path data-c=\"74\" d=\"M27 422Q80 426 109 478T141 600V615H181V431H316V385H181V241Q182 116 182 100T189 68Q203 29 238 29Q282 29 292 100Q293 108 293 146V181H333V146V134Q333 57 291 17Q264 -10 221 -10Q187 -10 162 2T124 33T105 68T98 100Q97 107 97 248V385H18V422H27Z\" transform=\"translate(1200,0)\" style=\"stroke-width: 3;\"></path><path data-c=\"6D\" d=\"M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z\" transform=\"translate(1589,0)\" style=\"stroke-width: 3;\"></path><path data-c=\"61\" d=\"M137 305T115 305T78 320T63 359Q63 394 97 421T218 448Q291 448 336 416T396 340Q401 326 401 309T402 194V124Q402 76 407 58T428 40Q443 40 448 56T453 109V145H493V106Q492 66 490 59Q481 29 455 12T400 -6T353 12T329 54V58L327 55Q325 52 322 49T314 40T302 29T287 17T269 6T247 -2T221 -8T190 -11Q130 -11 82 20T34 107Q34 128 41 147T68 188T116 225T194 253T304 268H318V290Q318 324 312 340Q290 411 215 411Q197 411 181 410T156 406T148 403Q170 388 170 359Q170 334 154 320ZM126 106Q126 75 150 51T209 26Q247 26 276 49T315 109Q317 116 318 175Q318 233 317 233Q309 233 296 232T251 223T193 203T147 166T126 106Z\" transform=\"translate(2422,0)\" style=\"stroke-width: 3;\"></path><path data-c=\"78\" d=\"M201 0Q189 3 102 3Q26 3 17 0H11V46H25Q48 47 67 52T96 61T121 78T139 96T160 122T180 150L226 210L168 288Q159 301 149 315T133 336T122 351T113 363T107 370T100 376T94 379T88 381T80 383Q74 383 44 385H16V431H23Q59 429 126 429Q219 429 229 431H237V385Q201 381 201 369Q201 367 211 353T239 315T268 274L272 270L297 304Q329 345 329 358Q329 364 327 369T322 376T317 380T310 384L307 385H302V431H309Q324 428 408 428Q487 428 493 431H499V385H492Q443 385 411 368Q394 360 377 341T312 257L296 236L358 151Q424 61 429 57T446 50Q464 46 499 46H516V0H510H502Q494 1 482 1T457 2T432 2T414 3Q403 3 377 3T327 1L304 0H295V46H298Q309 46 320 51T331 63Q331 65 291 120L250 175Q249 174 219 133T185 88Q181 83 181 74Q181 63 188 55T206 46Q208 46 208 23V0H201Z\" transform=\"translate(2922,0)\" style=\"stroke-width: 3;\"></path></g></g><g data-mml-node=\"mrow\" transform=\"translate(3616.7,0)\"><g data-mml-node=\"mo\" transform=\"translate(0 -0.5)\"><path data-c=\"28\" d=\"M180 96T180 250T205 541T266 770T353 944T444 1069T527 1150H555Q561 1144 561 1141Q561 1137 545 1120T504 1072T447 995T386 878T330 721T288 513T272 251Q272 133 280 56Q293 -87 326 -209T399 -405T475 -531T536 -609T561 -640Q561 -643 555 -649H527Q483 -612 443 -568T353 -443T266 -270T205 -41Z\" style=\"stroke-width: 3;\"></path></g><g data-mml-node=\"mfrac\" transform=\"translate(597,0)\"><g data-mml-node=\"mrow\" transform=\"translate(220,477.2) scale(0.707)\"><g data-mml-node=\"mi\"><path data-c=\"1D478\" d=\"M53 245Q53 297 70 356T125 478T216 590T349 671T523 703Q656 703 735 637T815 445Q815 410 808 370T781 277T729 178T643 87T519 14L525 4Q540 -19 553 -25T592 -32Q632 -32 654 -24T680 -7T689 10T704 18Q713 18 717 12T722 0Q722 -8 711 -36T681 -101T624 -166T541 -194Q513 -194 494 -183T465 -157T450 -118T444 -79T443 -41V-7L433 -9Q391 -17 344 -17Q301 -17 263 -10T185 15T118 62T71 138T53 245ZM666 482Q666 529 652 563T614 615T565 640T512 648Q412 648 335 573Q268 506 235 389T201 202Q201 164 210 136T230 95T259 66L262 76Q269 109 302 135T382 162Q401 162 415 159T449 140T484 92L491 78L496 82Q502 86 505 88T515 97T528 107T541 120T555 137T570 156T585 179T599 205T612 235Q629 278 647 351T666 482ZM439 56Q439 58 439 62T435 75T426 92T410 106T383 112Q353 112 332 96T311 63Q311 38 355 38H366Q391 39 415 45T439 56Z\" style=\"stroke-width: 3;\"></path></g><g data-mml-node=\"msup\" transform=\"translate(869,0)\"><g data-mml-node=\"mi\"><path data-c=\"1D472\" d=\"M536 0Q522 6 522 18Q522 35 533 57Q539 62 557 62Q595 62 601 65L472 330L365 255L342 160Q318 65 317 64Q317 62 361 62H388Q420 62 420 44Q416 6 403 2L399 0L357 1Q331 2 224 2Q149 2 109 1T65 0Q48 0 43 15Q47 54 60 60Q64 62 113 62H162L302 623Q302 624 258 624H235Q214 624 209 626T199 639Q203 678 216 684Q220 686 239 686Q290 684 403 684Q475 684 512 685T553 686Q576 686 576 668Q572 632 560 626Q555 624 506 624H457L422 481Q386 339 386 337L785 621Q779 624 749 624Q726 624 726 641Q726 645 730 659Q734 675 736 679T747 686L786 685Q812 684 888 684Q908 684 934 685T968 686Q1003 686 1003 669Q1003 646 991 629Q985 624 967 624Q918 624 888 617Q884 617 874 613L865 609Q864 608 732 515T599 420Q599 418 686 242T775 65Q784 62 829 62Q847 62 850 61T860 54Q862 52 862 43Q862 10 845 1Q844 1 842 1T836 0T797 1T694 2Q599 2 573 1L536 0Z\" style=\"stroke-width: 3;\"></path></g><g data-mml-node=\"mi\" transform=\"translate(1087.6,363) scale(0.707)\"><path data-c=\"22A4\" d=\"M55 642T55 648T59 659T66 666T71 668H708Q723 660 723 648T708 628H409V15Q402 2 391 0Q387 0 384 1T379 3T375 6T373 9T371 13T369 16V628H71Q70 628 67 630T59 637Z\" style=\"stroke-width: 3;\"></path></g></g></g><g data-mml-node=\"msqrt\" transform=\"translate(638.5,-525.5) scale(0.707)\"><g transform=\"translate(853,0)\"><g data-mml-node=\"mi\"><path data-c=\"1D451\" d=\"M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z\" style=\"stroke-width: 3;\"></path></g></g><g data-mml-node=\"mo\" transform=\"translate(0,91.9)\"><path data-c=\"221A\" d=\"M95 178Q89 178 81 186T72 200T103 230T169 280T207 309Q209 311 212 311H213Q219 311 227 294T281 177Q300 134 312 108L397 -77Q398 -77 501 136T707 565T814 786Q820 800 834 800Q841 800 846 794T853 782V776L620 293L385 -193Q381 -200 366 -200Q357 -200 354 -197Q352 -195 256 15L160 225L144 214Q129 202 113 190T95 178Z\" style=\"stroke-width: 3;\"></path></g><rect width=\"520\" height=\"42.4\" x=\"853\" y=\"849.5\"></rect></g><rect width=\"2007.9\" height=\"60\" x=\"120\" y=\"220\"></rect></g><g data-mml-node=\"mo\" transform=\"translate(2844.9,0) translate(0 -0.5)\"><path data-c=\"29\" d=\"M35 1138Q35 1150 51 1150H56H69Q113 1113 153 1069T243 944T330 771T391 541T416 250T391 -40T330 -270T243 -443T152 -568T69 -649H56Q43 -649 39 -647T35 -637Q65 -607 110 -548Q283 -316 316 56Q324 133 324 251Q324 368 316 445Q278 877 48 1123Q36 1137 35 1138Z\" style=\"stroke-width: 3;\"></path></g></g></g></g></svg><mjx-assistive-mml unselectable=\"on\" display=\"inline\" style=\"top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow data-mjx-texclass=\"ORD\"><mi data-mjx-auto-op=\"false\">softmax</mi></mrow><mrow data-mjx-texclass=\"INNER\"><mo data-mjx-texclass=\"OPEN\">(</mo><mfrac><mrow><mi mathvariant=\"bold-italic\">Q</mi><msup><mi mathvariant=\"bold-italic\">K</mi><mi mathvariant=\"normal\">⊤</mi></msup></mrow><msqrt><mi>d</mi></msqrt></mfrac><mo data-mjx-texclass=\"CLOSE\">)</mo></mrow></math></mjx-assistive-mml></mjx-container> often described as the attention matrix <mjx-container class=\"MathJax\" jax=\"SVG\" style=\"direction: ltr; position: relative;\"><svg style=\"overflow: visible; min-height: 1px; min-width: 1px; vertical-align: -0.09ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"9.718ex\" height=\"1.699ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -711 4295.2 751\" aria-hidden=\"true\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"mi\"><path data-c=\"1D468\" d=\"M65 0Q45 0 45 18Q48 52 61 60Q65 62 81 62Q155 62 165 74Q166 74 265 228T465 539T569 699Q576 707 583 709T611 711T637 710T649 700Q650 697 695 380L741 63L784 62H827Q839 50 839 45L835 29Q831 9 827 5T806 0Q803 0 790 0T743 1T657 2Q585 2 547 1T504 0Q481 0 481 17Q484 54 497 60Q501 62 541 62Q580 62 580 63Q580 68 573 121T564 179V181H308L271 124Q236 69 236 67T283 62H287Q316 62 316 46Q316 26 307 8Q302 3 295 0L262 1Q242 2 168 2Q119 2 93 1T65 0ZM537 372Q533 402 528 435T521 486T518 504V505Q517 505 433 375L348 244L451 243Q555 243 555 244L537 372Z\" style=\"stroke-width: 3;\"></path></g><g data-mml-node=\"mo\" transform=\"translate(1146.8,0)\"><path data-c=\"2208\" d=\"M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z\" style=\"stroke-width: 3;\"></path></g><g data-mml-node=\"msup\" transform=\"translate(2091.6,0)\"><g data-mml-node=\"TeXAtom\" data-mjx-texclass=\"ORD\"><g data-mml-node=\"mi\"><path data-c=\"211D\" d=\"M17 665Q17 672 28 683H221Q415 681 439 677Q461 673 481 667T516 654T544 639T566 623T584 607T597 592T607 578T614 565T618 554L621 548Q626 530 626 497Q626 447 613 419Q578 348 473 326L455 321Q462 310 473 292T517 226T578 141T637 72T686 35Q705 30 705 16Q705 7 693 -1H510Q503 6 404 159L306 310H268V183Q270 67 271 59Q274 42 291 38Q295 37 319 35Q344 35 353 28Q362 17 353 3L346 -1H28Q16 5 16 16Q16 35 55 35Q96 38 101 52Q106 60 106 341T101 632Q95 645 55 648Q17 648 17 665ZM241 35Q238 42 237 45T235 78T233 163T233 337V621L237 635L244 648H133Q136 641 137 638T139 603T141 517T141 341Q141 131 140 89T134 37Q133 36 133 35H241ZM457 496Q457 540 449 570T425 615T400 634T377 643Q374 643 339 648Q300 648 281 635Q271 628 270 610T268 481V346H284Q327 346 375 352Q421 364 439 392T457 496ZM492 537T492 496T488 427T478 389T469 371T464 361Q464 360 465 360Q469 360 497 370Q593 400 593 495Q593 592 477 630L457 637L461 626Q474 611 488 561Q492 537 492 496ZM464 243Q411 317 410 317Q404 317 401 315Q384 315 370 312H346L526 35H619L606 50Q553 109 464 243Z\" style=\"stroke-width: 3;\"></path></g></g><g data-mml-node=\"TeXAtom\" transform=\"translate(755,363) scale(0.707)\" data-mjx-texclass=\"ORD\"><g data-mml-node=\"mi\"><path data-c=\"1D45B\" d=\"M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z\" style=\"stroke-width: 3;\"></path></g><g data-mml-node=\"mo\" transform=\"translate(600,0)\"><path data-c=\"D7\" d=\"M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z\" style=\"stroke-width: 3;\"></path></g><g data-mml-node=\"mi\" transform=\"translate(1378,0)\"><path data-c=\"1D45B\" d=\"M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z\" style=\"stroke-width: 3;\"></path></g></g></g></g></g></svg><mjx-assistive-mml unselectable=\"on\" display=\"inline\" style=\"top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi mathvariant=\"bold-italic\">A</mi><mo>∈</mo><msup><mrow data-mjx-texclass=\"ORD\"><mi mathvariant=\"double-struck\">R</mi></mrow><mrow data-mjx-texclass=\"ORD\"><mi>n</mi><mo>×</mo><mi>n</mi></mrow></msup></math></mjx-assistive-mml></mjx-container>. Denoting <mjx-container class=\"MathJax\" jax=\"SVG\" style=\"direction: ltr; position: relative;\"><svg style=\"overflow: visible; min-height: 1px; min-width: 1px; vertical-align: -1.469ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"20.726ex\" height=\"4.07ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -1149.5 9161.1 1799\" aria-hidden=\"true\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"mi\"><path data-c=\"1D453\" d=\"M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z\" style=\"stroke-width: 3;\"></path></g><g data-mml-node=\"mo\" transform=\"translate(550,0)\"><path data-c=\"28\" d=\"M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z\" style=\"stroke-width: 3;\"></path></g><g data-mml-node=\"mo\" transform=\"translate(939,0)\"><path data-c=\"22C5\" d=\"M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z\" style=\"stroke-width: 3;\"></path></g><g data-mml-node=\"mo\" transform=\"translate(1217,0)\"><path data-c=\"29\" d=\"M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z\" style=\"stroke-width: 3;\"></path></g><g data-mml-node=\"mo\" transform=\"translate(1883.8,0)\"><path data-c=\"3D\" d=\"M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z\" style=\"stroke-width: 3;\"></path></g><g data-mml-node=\"TeXAtom\" data-mjx-texclass=\"ORD\" transform=\"translate(2939.6,0)\"><g data-mml-node=\"mi\"><path data-c=\"73\" d=\"M295 316Q295 356 268 385T190 414Q154 414 128 401Q98 382 98 349Q97 344 98 336T114 312T157 287Q175 282 201 278T245 269T277 256Q294 248 310 236T342 195T359 133Q359 71 321 31T198 -10H190Q138 -10 94 26L86 19L77 10Q71 4 65 -1L54 -11H46H42Q39 -11 33 -5V74V132Q33 153 35 157T45 162H54Q66 162 70 158T75 146T82 119T101 77Q136 26 198 26Q295 26 295 104Q295 133 277 151Q257 175 194 187T111 210Q75 227 54 256T33 318Q33 357 50 384T93 424T143 442T187 447H198Q238 447 268 432L283 424L292 431Q302 440 314 448H322H326Q329 448 335 442V310L329 304H301Q295 310 295 316Z\" style=\"stroke-width: 3;\"></path><path data-c=\"6F\" d=\"M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z\" transform=\"translate(394,0)\" style=\"stroke-width: 3;\"></path><path data-c=\"66\" d=\"M273 0Q255 3 146 3Q43 3 34 0H26V46H42Q70 46 91 49Q99 52 103 60Q104 62 104 224V385H33V431H104V497L105 564L107 574Q126 639 171 668T266 704Q267 704 275 704T289 705Q330 702 351 679T372 627Q372 604 358 590T321 576T284 590T270 627Q270 647 288 667H284Q280 668 273 668Q245 668 223 647T189 592Q183 572 182 497V431H293V385H185V225Q185 63 186 61T189 57T194 54T199 51T206 49T213 48T222 47T231 47T241 46T251 46H282V0H273Z\" transform=\"translate(894,0)\" style=\"stroke-width: 3;\"></path><path data-c=\"74\" d=\"M27 422Q80 426 109 478T141 600V615H181V431H316V385H181V241Q182 116 182 100T189 68Q203 29 238 29Q282 29 292 100Q293 108 293 146V181H333V146V134Q333 57 291 17Q264 -10 221 -10Q187 -10 162 2T124 33T105 68T98 100Q97 107 97 248V385H18V422H27Z\" transform=\"translate(1200,0)\" style=\"stroke-width: 3;\"></path><path data-c=\"6D\" d=\"M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z\" transform=\"translate(1589,0)\" style=\"stroke-width: 3;\"></path><path data-c=\"61\" d=\"M137 305T115 305T78 320T63 359Q63 394 97 421T218 448Q291 448 336 416T396 340Q401 326 401 309T402 194V124Q402 76 407 58T428 40Q443 40 448 56T453 109V145H493V106Q492 66 490 59Q481 29 455 12T400 -6T353 12T329 54V58L327 55Q325 52 322 49T314 40T302 29T287 17T269 6T247 -2T221 -8T190 -11Q130 -11 82 20T34 107Q34 128 41 147T68 188T116 225T194 253T304 268H318V290Q318 324 312 340Q290 411 215 411Q197 411 181 410T156 406T148 403Q170 388 170 359Q170 334 154 320ZM126 106Q126 75 150 51T209 26Q247 26 276 49T315 109Q317 116 318 175Q318 233 317 233Q309 233 296 232T251 223T193 203T147 166T126 106Z\" transform=\"translate(2422,0)\" style=\"stroke-width: 3;\"></path><path data-c=\"78\" d=\"M201 0Q189 3 102 3Q26 3 17 0H11V46H25Q48 47 67 52T96 61T121 78T139 96T160 122T180 150L226 210L168 288Q159 301 149 315T133 336T122 351T113 363T107 370T100 376T94 379T88 381T80 383Q74 383 44 385H16V431H23Q59 429 126 429Q219 429 229 431H237V385Q201 381 201 369Q201 367 211 353T239 315T268 274L272 270L297 304Q329 345 329 358Q329 364 327 369T322 376T317 380T310 384L307 385H302V431H309Q324 428 408 428Q487 428 493 431H499V385H492Q443 385 411 368Q394 360 377 341T312 257L296 236L358 151Q424 61 429 57T446 50Q464 46 499 46H516V0H510H502Q494 1 482 1T457 2T432 2T414 3Q403 3 377 3T327 1L304 0H295V46H298Q309 46 320 51T331 63Q331 65 291 120L250 175Q249 174 219 133T185 88Q181 83 181 74Q181 63 188 55T206 46Q208 46 208 23V0H201Z\" transform=\"translate(2922,0)\" style=\"stroke-width: 3;\"></path></g></g><g data-mml-node=\"mrow\" transform=\"translate(6556.2,0)\"><g data-mml-node=\"mo\" transform=\"translate(0 -0.5)\"><path data-c=\"28\" d=\"M180 96T180 250T205 541T266 770T353 944T444 1069T527 1150H555Q561 1144 561 1141Q561 1137 545 1120T504 1072T447 995T386 878T330 721T288 513T272 251Q272 133 280 56Q293 -87 326 -209T399 -405T475 -531T536 -609T561 -640Q561 -643 555 -649H527Q483 -612 443 -568T353 -443T266 -270T205 -41Z\" style=\"stroke-width: 3;\"></path></g><g data-mml-node=\"mfrac\" transform=\"translate(597,0)\"><g data-mml-node=\"mo\" transform=\"translate(607.1,394) scale(0.707)\"><path data-c=\"22C5\" d=\"M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z\" style=\"stroke-width: 3;\"></path></g><g data-mml-node=\"msqrt\" transform=\"translate(220,-525.5) scale(0.707)\"><g transform=\"translate(853,0)\"><g data-mml-node=\"mi\"><path data-c=\"1D451\" d=\"M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z\" style=\"stroke-width: 3;\"></path></g></g><g data-mml-node=\"mo\" transform=\"translate(0,91.9)\"><path data-c=\"221A\" d=\"M95 178Q89 178 81 186T72 200T103 230T169 280T207 309Q209 311 212 311H213Q219 311 227 294T281 177Q300 134 312 108L397 -77Q398 -77 501 136T707 565T814 786Q820 800 834 800Q841 800 846 794T853 782V776L620 293L385 -193Q381 -200 366 -200Q357 -200 354 -197Q352 -195 256 15L160 225L144 214Q129 202 113 190T95 178Z\" style=\"stroke-width: 3;\"></path></g><rect width=\"520\" height=\"42.4\" x=\"853\" y=\"849.5\"></rect></g><rect width=\"1170.9\" height=\"60\" x=\"120\" y=\"220\"></rect></g><g data-mml-node=\"mo\" transform=\"translate(2007.9,0) translate(0 -0.5)\"><path data-c=\"29\" d=\"M35 1138Q35 1150 51 1150H56H69Q113 1113 153 1069T243 944T330 771T391 541T416 250T391 -40T330 -270T243 -443T152 -568T69 -649H56Q43 -649 39 -647T35 -637Q65 -607 110 -548Q283 -316 316 56Q324 133 324 251Q324 368 316 445Q278 877 48 1123Q36 1137 35 1138Z\" style=\"stroke-width: 3;\"></path></g></g></g></g></svg><mjx-assistive-mml unselectable=\"on\" display=\"inline\" style=\"top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>f</mi><mo stretchy=\"false\">(</mo><mo>⋅</mo><mo stretchy=\"false\">)</mo><mo>=</mo><mrow data-mjx-texclass=\"ORD\"><mi data-mjx-auto-op=\"false\">softmax</mi></mrow><mrow data-mjx-texclass=\"INNER\"><mo data-mjx-texclass=\"OPEN\">(</mo><mfrac><mo>⋅</mo><msqrt><mi>d</mi></msqrt></mfrac><mo data-mjx-texclass=\"CLOSE\">)</mo></mrow></math></mjx-assistive-mml></mjx-container> as a normalizing function, one may re-write the whole operation as:</p>\n<mjx-container class=\"MathJax\" jax=\"SVG\" display=\"true\" style=\"direction: ltr; display: block; text-align: center; margin: 1em 0; position: relative;\"><svg style=\"overflow: visible; min-height: 1px; min-width: 1px; vertical-align: -1.469ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"28.731ex\" height=\"4.07ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -1149.5 12699.3 1799\" aria-hidden=\"true\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"mi\"><path data-c=\"1D481\" d=\"M223 430Q192 430 192 448Q192 450 225 561T261 677Q265 683 270 684Q273 686 534 686Q796 686 797 685Q805 682 805 673Q805 668 804 661T800 648T798 641Q796 637 531 352L266 67L329 66H364Q412 66 446 70T523 96T596 157Q617 186 630 220T649 273T663 297Q667 299 684 299H688Q715 299 715 281Q715 278 673 145T628 8Q626 4 617 0H348Q289 0 221 0T139 -1Q112 -1 99 -1T78 1T69 5T68 12Q68 16 71 31T77 49L84 57Q91 65 104 79T133 110T170 151T213 196L610 624H540Q533 624 514 624T488 624T467 623T443 620T422 616T398 609T373 600Q292 560 255 449Q251 436 246 433T223 430Z\" style=\"stroke-width: 3;\"></path></g><g data-mml-node=\"mo\" transform=\"translate(1082.8,0)\"><path data-c=\"3D\" d=\"M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z\" style=\"stroke-width: 3;\"></path></g><g data-mml-node=\"mi\" transform=\"translate(2138.6,0)\"><path data-c=\"1D453\" d=\"M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z\" style=\"stroke-width: 3;\"></path></g><g data-mml-node=\"TeXAtom\" data-mjx-texclass=\"ORD\" transform=\"translate(2688.6,0)\"><g data-mml-node=\"mo\" transform=\"translate(0 -0.5)\"><path data-c=\"28\" d=\"M180 96T180 250T205 541T266 770T353 944T444 1069T527 1150H555Q561 1144 561 1141Q561 1137 545 1120T504 1072T447 995T386 878T330 721T288 513T272 251Q272 133 280 56Q293 -87 326 -209T399 -405T475 -531T536 -609T561 -640Q561 -643 555 -649H527Q483 -612 443 -568T353 -443T266 -270T205 -41Z\" style=\"stroke-width: 3;\"></path></g></g><g data-mml-node=\"mi\" transform=\"translate(3285.6,0)\"><path data-c=\"1D47F\" d=\"M931 686Q953 686 953 670Q953 650 944 632Q936 624 924 624H914Q823 624 803 611Q800 609 696 503T591 396Q591 394 667 229L743 62H787H814Q846 62 846 44Q843 7 829 2Q825 0 817 0Q813 0 775 1T664 2Q590 2 551 1T508 0H507Q484 0 484 18Q484 19 488 37Q492 56 497 58T534 62L566 63Q567 64 520 169T471 274Q469 274 369 172T268 67L315 62Q320 62 328 62L335 61Q347 58 347 44Q344 10 331 2L326 0L287 1Q263 2 177 2Q95 2 78 1L53 0Q38 6 38 17Q38 40 50 57Q56 62 78 62Q169 62 188 75Q194 77 435 324L444 334L439 347Q437 351 373 492L313 624H268H246Q220 624 212 632Q210 636 210 642Q210 655 215 669T227 684Q230 686 247 686Q295 684 398 684Q438 684 472 684T527 685T551 686Q567 686 572 671Q572 667 568 651Q563 631 558 628T523 624T492 623H488L526 540Q563 457 564 457Q564 456 574 466T604 496T645 537L724 619Q716 622 677 624H673Q645 624 645 640Q645 660 654 678Q659 683 666 686L704 685Q728 684 813 684Q847 684 873 684T913 685T931 686Z\" style=\"stroke-width: 3;\"></path></g><g data-mml-node=\"msub\" transform=\"translate(4232.6,0)\"><g data-mml-node=\"mi\"><path data-c=\"1D47E\" d=\"M111 624Q109 624 102 624T91 623Q61 623 61 640Q61 660 70 678Q78 686 98 686Q140 684 239 684Q277 684 309 684T360 685T383 686H385Q407 686 407 668Q404 634 391 626Q387 624 348 624Q307 624 307 622Q307 618 332 409Q359 198 359 195L570 532L564 576L558 622V624H522H504Q472 624 472 641Q475 678 488 684L493 686L529 685Q551 684 645 684Q716 684 753 685T795 686Q818 686 818 669Q815 632 802 626Q798 624 759 624Q718 624 718 622Q718 615 743 410Q770 199 770 196Q770 195 806 253T903 406Q1035 618 1035 619Q1025 624 968 624Q943 624 943 641Q943 648 946 659Q950 675 952 679T963 686L998 685Q1020 684 1093 684Q1113 684 1139 685T1173 686Q1207 686 1207 669Q1207 664 1204 652Q1199 631 1194 628T1164 624Q1113 622 1101 615Q1098 612 905 305Q715 -1 709 -7Q699 -17 673 -17Q645 -17 639 -8L581 441Q581 444 442 221Q331 44 314 18T288 -14Q279 -17 263 -17H254Q229 -17 227 -5Q225 2 186 311L147 620V624H111Z\" style=\"stroke-width: 3;\"></path></g><g data-mml-node=\"mi\" transform=\"translate(1126,-150) scale(0.707)\"><path data-c=\"1D45E\" d=\"M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z\" style=\"stroke-width: 3;\"></path></g></g><g data-mml-node=\"mo\" transform=\"translate(5733.8,0)\"><path data-c=\"28\" d=\"M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z\" style=\"stroke-width: 3;\"></path></g><g data-mml-node=\"mi\" transform=\"translate(6122.8,0)\"><path data-c=\"1D47F\" d=\"M931 686Q953 686 953 670Q953 650 944 632Q936 624 924 624H914Q823 624 803 611Q800 609 696 503T591 396Q591 394 667 229L743 62H787H814Q846 62 846 44Q843 7 829 2Q825 0 817 0Q813 0 775 1T664 2Q590 2 551 1T508 0H507Q484 0 484 18Q484 19 488 37Q492 56 497 58T534 62L566 63Q567 64 520 169T471 274Q469 274 369 172T268 67L315 62Q320 62 328 62L335 61Q347 58 347 44Q344 10 331 2L326 0L287 1Q263 2 177 2Q95 2 78 1L53 0Q38 6 38 17Q38 40 50 57Q56 62 78 62Q169 62 188 75Q194 77 435 324L444 334L439 347Q437 351 373 492L313 624H268H246Q220 624 212 632Q210 636 210 642Q210 655 215 669T227 684Q230 686 247 686Q295 684 398 684Q438 684 472 684T527 685T551 686Q567 686 572 671Q572 667 568 651Q563 631 558 628T523 624T492 623H488L526 540Q563 457 564 457Q564 456 574 466T604 496T645 537L724 619Q716 622 677 624H673Q645 624 645 640Q645 660 654 678Q659 683 666 686L704 685Q728 684 813 684Q847 684 873 684T913 685T931 686Z\" style=\"stroke-width: 3;\"></path></g><g data-mml-node=\"msub\" transform=\"translate(7069.8,0)\"><g data-mml-node=\"mi\"><path data-c=\"1D47E\" d=\"M111 624Q109 624 102 624T91 623Q61 623 61 640Q61 660 70 678Q78 686 98 686Q140 684 239 684Q277 684 309 684T360 685T383 686H385Q407 686 407 668Q404 634 391 626Q387 624 348 624Q307 624 307 622Q307 618 332 409Q359 198 359 195L570 532L564 576L558 622V624H522H504Q472 624 472 641Q475 678 488 684L493 686L529 685Q551 684 645 684Q716 684 753 685T795 686Q818 686 818 669Q815 632 802 626Q798 624 759 624Q718 624 718 622Q718 615 743 410Q770 199 770 196Q770 195 806 253T903 406Q1035 618 1035 619Q1025 624 968 624Q943 624 943 641Q943 648 946 659Q950 675 952 679T963 686L998 685Q1020 684 1093 684Q1113 684 1139 685T1173 686Q1207 686 1207 669Q1207 664 1204 652Q1199 631 1194 628T1164 624Q1113 622 1101 615Q1098 612 905 305Q715 -1 709 -7Q699 -17 673 -17Q645 -17 639 -8L581 441Q581 444 442 221Q331 44 314 18T288 -14Q279 -17 263 -17H254Q229 -17 227 -5Q225 2 186 311L147 620V624H111Z\" style=\"stroke-width: 3;\"></path></g><g data-mml-node=\"mi\" transform=\"translate(1126,-150) scale(0.707)\"><path data-c=\"1D458\" d=\"M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z\" style=\"stroke-width: 3;\"></path></g></g><g data-mml-node=\"msup\" transform=\"translate(8614.2,0)\"><g data-mml-node=\"mo\"><path data-c=\"29\" d=\"M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z\" style=\"stroke-width: 3;\"></path></g><g data-mml-node=\"mi\" transform=\"translate(422,413) scale(0.707)\"><path data-c=\"22A4\" d=\"M55 642T55 648T59 659T66 666T71 668H708Q723 660 723 648T708 628H409V15Q402 2 391 0Q387 0 384 1T379 3T375 6T373 9T371 13T369 16V628H71Q70 628 67 630T59 637Z\" style=\"stroke-width: 3;\"></path></g></g><g data-mml-node=\"TeXAtom\" data-mjx-texclass=\"ORD\" transform=\"translate(9636.4,0)\"><g data-mml-node=\"mo\" transform=\"translate(0 -0.5)\"><path data-c=\"29\" d=\"M35 1138Q35 1150 51 1150H56H69Q113 1113 153 1069T243 944T330 771T391 541T416 250T391 -40T330 -270T243 -443T152 -568T69 -649H56Q43 -649 39 -647T35 -637Q65 -607 110 -548Q283 -316 316 56Q324 133 324 251Q324 368 316 445Q278 877 48 1123Q36 1137 35 1138Z\" style=\"stroke-width: 3;\"></path></g></g><g data-mml-node=\"mi\" transform=\"translate(10233.4,0)\"><path data-c=\"1D47F\" d=\"M931 686Q953 686 953 670Q953 650 944 632Q936 624 924 624H914Q823 624 803 611Q800 609 696 503T591 396Q591 394 667 229L743 62H787H814Q846 62 846 44Q843 7 829 2Q825 0 817 0Q813 0 775 1T664 2Q590 2 551 1T508 0H507Q484 0 484 18Q484 19 488 37Q492 56 497 58T534 62L566 63Q567 64 520 169T471 274Q469 274 369 172T268 67L315 62Q320 62 328 62L335 61Q347 58 347 44Q344 10 331 2L326 0L287 1Q263 2 177 2Q95 2 78 1L53 0Q38 6 38 17Q38 40 50 57Q56 62 78 62Q169 62 188 75Q194 77 435 324L444 334L439 347Q437 351 373 492L313 624H268H246Q220 624 212 632Q210 636 210 642Q210 655 215 669T227 684Q230 686 247 686Q295 684 398 684Q438 684 472 684T527 685T551 686Q567 686 572 671Q572 667 568 651Q563 631 558 628T523 624T492 623H488L526 540Q563 457 564 457Q564 456 574 466T604 496T645 537L724 619Q716 622 677 624H673Q645 624 645 640Q645 660 654 678Q659 683 666 686L704 685Q728 684 813 684Q847 684 873 684T913 685T931 686Z\" style=\"stroke-width: 3;\"></path></g><g data-mml-node=\"msub\" transform=\"translate(11180.4,0)\"><g data-mml-node=\"mi\"><path data-c=\"1D47E\" d=\"M111 624Q109 624 102 624T91 623Q61 623 61 640Q61 660 70 678Q78 686 98 686Q140 684 239 684Q277 684 309 684T360 685T383 686H385Q407 686 407 668Q404 634 391 626Q387 624 348 624Q307 624 307 622Q307 618 332 409Q359 198 359 195L570 532L564 576L558 622V624H522H504Q472 624 472 641Q475 678 488 684L493 686L529 685Q551 684 645 684Q716 684 753 685T795 686Q818 686 818 669Q815 632 802 626Q798 624 759 624Q718 624 718 622Q718 615 743 410Q770 199 770 196Q770 195 806 253T903 406Q1035 618 1035 619Q1025 624 968 624Q943 624 943 641Q943 648 946 659Q950 675 952 679T963 686L998 685Q1020 684 1093 684Q1113 684 1139 685T1173 686Q1207 686 1207 669Q1207 664 1204 652Q1199 631 1194 628T1164 624Q1113 622 1101 615Q1098 612 905 305Q715 -1 709 -7Q699 -17 673 -17Q645 -17 639 -8L581 441Q581 444 442 221Q331 44 314 18T288 -14Q279 -17 263 -17H254Q229 -17 227 -5Q225 2 186 311L147 620V624H111Z\" style=\"stroke-width: 3;\"></path></g><g data-mml-node=\"mi\" transform=\"translate(1126,-150) scale(0.707)\"><path data-c=\"1D463\" d=\"M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z\" style=\"stroke-width: 3;\"></path></g></g></g></g></svg><mjx-assistive-mml unselectable=\"on\" display=\"block\" style=\"top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; overflow: hidden; width: 100%;\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi mathvariant=\"bold-italic\">Z</mi><mo>=</mo><mi>f</mi><mrow data-mjx-texclass=\"ORD\"><mo minsize=\"1.623em\" maxsize=\"1.623em\">(</mo></mrow><mi mathvariant=\"bold-italic\">X</mi><msub><mi mathvariant=\"bold-italic\">W</mi><mi>q</mi></msub><mo stretchy=\"false\">(</mo><mi mathvariant=\"bold-italic\">X</mi><msub><mi mathvariant=\"bold-italic\">W</mi><mi>k</mi></msub><msup><mo stretchy=\"false\">)</mo><mi mathvariant=\"normal\">⊤</mi></msup><mrow data-mjx-texclass=\"ORD\"><mo minsize=\"1.623em\" maxsize=\"1.623em\">)</mo></mrow><mi mathvariant=\"bold-italic\">X</mi><msub><mi mathvariant=\"bold-italic\">W</mi><mi>v</mi></msub></math></mjx-assistive-mml></mjx-container><p>Via this equation, one sees that, intuitively, self-attention is nothing more than three projections of <mjx-container class=\"MathJax\" jax=\"SVG\" style=\"direction: ltr; position: relative;\"><svg style=\"overflow: visible; min-height: 1px; min-width: 1px; vertical-align: 0;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"2.143ex\" height=\"1.552ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -686 947 686\" aria-hidden=\"true\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"mi\"><path data-c=\"1D47F\" d=\"M931 686Q953 686 953 670Q953 650 944 632Q936 624 924 624H914Q823 624 803 611Q800 609 696 503T591 396Q591 394 667 229L743 62H787H814Q846 62 846 44Q843 7 829 2Q825 0 817 0Q813 0 775 1T664 2Q590 2 551 1T508 0H507Q484 0 484 18Q484 19 488 37Q492 56 497 58T534 62L566 63Q567 64 520 169T471 274Q469 274 369 172T268 67L315 62Q320 62 328 62L335 61Q347 58 347 44Q344 10 331 2L326 0L287 1Q263 2 177 2Q95 2 78 1L53 0Q38 6 38 17Q38 40 50 57Q56 62 78 62Q169 62 188 75Q194 77 435 324L444 334L439 347Q437 351 373 492L313 624H268H246Q220 624 212 632Q210 636 210 642Q210 655 215 669T227 684Q230 686 247 686Q295 684 398 684Q438 684 472 684T527 685T551 686Q567 686 572 671Q572 667 568 651Q563 631 558 628T523 624T492 623H488L526 540Q563 457 564 457Q564 456 574 466T604 496T645 537L724 619Q716 622 677 624H673Q645 624 645 640Q645 660 654 678Q659 683 666 686L704 685Q728 684 813 684Q847 684 873 684T913 685T931 686Z\" style=\"stroke-width: 3;\"></path></g></g></g></svg><mjx-assistive-mml unselectable=\"on\" display=\"inline\" style=\"top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi mathvariant=\"bold-italic\">X</mi></math></mjx-assistive-mml></mjx-container> multiplied with eachother, with a normalization function in between. Self-attention may be visualized as:</p>\n<div style=\"text-align: center;\" id=\"vis-attn\"></div>\n<p style=\"text-align: center;\">\n  <i><small>(Hover over the elements highlighted in blue to see computation)</small></i>\n</p>\n<p>For the purpose of the visualizations, I've conveniently ignored the multiple heads that are typically used with self-attention.</p>\n<h2 id=\"permutation-equivariance-in-self-attention\" tabindex=\"-1\">Permutation equivariance in self-attention <a class=\"header-anchor\" href=\"https://gdewael.github.io/blog/equivariance/\">#</a></h2>\n<p>The words in the previous example sentence &quot;Love conquers all&quot; can be scrambled in a number of ways, and still form a correct sentence, e.g. &quot;All love conquers&quot;.\nWe can play around with the previous visualizations of self-attention by adding a shuffle button:</p>\n<div style=\"text-align: center;\" id=\"vis-shuffle\"></div>\n<p>If you play around with the shuffling, you will notice that, if elements of <mjx-container class=\"MathJax\" jax=\"SVG\" style=\"direction: ltr; position: relative;\"><svg style=\"overflow: visible; min-height: 1px; min-width: 1px; vertical-align: 0;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"2.143ex\" height=\"1.552ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -686 947 686\" aria-hidden=\"true\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"mi\"><path data-c=\"1D47F\" d=\"M931 686Q953 686 953 670Q953 650 944 632Q936 624 924 624H914Q823 624 803 611Q800 609 696 503T591 396Q591 394 667 229L743 62H787H814Q846 62 846 44Q843 7 829 2Q825 0 817 0Q813 0 775 1T664 2Q590 2 551 1T508 0H507Q484 0 484 18Q484 19 488 37Q492 56 497 58T534 62L566 63Q567 64 520 169T471 274Q469 274 369 172T268 67L315 62Q320 62 328 62L335 61Q347 58 347 44Q344 10 331 2L326 0L287 1Q263 2 177 2Q95 2 78 1L53 0Q38 6 38 17Q38 40 50 57Q56 62 78 62Q169 62 188 75Q194 77 435 324L444 334L439 347Q437 351 373 492L313 624H268H246Q220 624 212 632Q210 636 210 642Q210 655 215 669T227 684Q230 686 247 686Q295 684 398 684Q438 684 472 684T527 685T551 686Q567 686 572 671Q572 667 568 651Q563 631 558 628T523 624T492 623H488L526 540Q563 457 564 457Q564 456 574 466T604 496T645 537L724 619Q716 622 677 624H673Q645 624 645 640Q645 660 654 678Q659 683 666 686L704 685Q728 684 813 684Q847 684 873 684T913 685T931 686Z\" style=\"stroke-width: 3;\"></path></g></g></g></svg><mjx-assistive-mml unselectable=\"on\" display=\"inline\" style=\"top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi mathvariant=\"bold-italic\">X</mi></math></mjx-assistive-mml></mjx-container> are reordered, the output <mjx-container class=\"MathJax\" jax=\"SVG\" style=\"direction: ltr; position: relative;\"><svg style=\"overflow: visible; min-height: 1px; min-width: 1px; vertical-align: 0;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"1.821ex\" height=\"1.552ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -686 805 686\" aria-hidden=\"true\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"mi\"><path data-c=\"1D481\" d=\"M223 430Q192 430 192 448Q192 450 225 561T261 677Q265 683 270 684Q273 686 534 686Q796 686 797 685Q805 682 805 673Q805 668 804 661T800 648T798 641Q796 637 531 352L266 67L329 66H364Q412 66 446 70T523 96T596 157Q617 186 630 220T649 273T663 297Q667 299 684 299H688Q715 299 715 281Q715 278 673 145T628 8Q626 4 617 0H348Q289 0 221 0T139 -1Q112 -1 99 -1T78 1T69 5T68 12Q68 16 71 31T77 49L84 57Q91 65 104 79T133 110T170 151T213 196L610 624H540Q533 624 514 624T488 624T467 623T443 620T422 616T398 609T373 600Q292 560 255 449Q251 436 246 433T223 430Z\" style=\"stroke-width: 3;\"></path></g></g></g></svg><mjx-assistive-mml unselectable=\"on\" display=\"inline\" style=\"top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi mathvariant=\"bold-italic\">Z</mi></math></mjx-assistive-mml></mjx-container> is similarly reordered without otherwise changing.\nWithin the framework of geometric deep learning, this property is called <em>permutation equivariance</em>.\nMore formally, with the self-attention function denoted as <mjx-container class=\"MathJax\" jax=\"SVG\" style=\"direction: ltr; position: relative;\"><svg style=\"overflow: visible; min-height: 1px; min-width: 1px; vertical-align: -0.464ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"4.23ex\" height=\"2.059ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -705 1869.8 910\" aria-hidden=\"true\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"msub\"><g data-mml-node=\"mi\"><path data-c=\"1D453\" d=\"M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z\" style=\"stroke-width: 3;\"></path></g><g data-mml-node=\"mtext\" transform=\"translate(523,-150) scale(0.707)\"><path data-c=\"61\" d=\"M137 305T115 305T78 320T63 359Q63 394 97 421T218 448Q291 448 336 416T396 340Q401 326 401 309T402 194V124Q402 76 407 58T428 40Q443 40 448 56T453 109V145H493V106Q492 66 490 59Q481 29 455 12T400 -6T353 12T329 54V58L327 55Q325 52 322 49T314 40T302 29T287 17T269 6T247 -2T221 -8T190 -11Q130 -11 82 20T34 107Q34 128 41 147T68 188T116 225T194 253T304 268H318V290Q318 324 312 340Q290 411 215 411Q197 411 181 410T156 406T148 403Q170 388 170 359Q170 334 154 320ZM126 106Q126 75 150 51T209 26Q247 26 276 49T315 109Q317 116 318 175Q318 233 317 233Q309 233 296 232T251 223T193 203T147 166T126 106Z\" style=\"stroke-width: 3;\"></path><path data-c=\"74\" d=\"M27 422Q80 426 109 478T141 600V615H181V431H316V385H181V241Q182 116 182 100T189 68Q203 29 238 29Q282 29 292 100Q293 108 293 146V181H333V146V134Q333 57 291 17Q264 -10 221 -10Q187 -10 162 2T124 33T105 68T98 100Q97 107 97 248V385H18V422H27Z\" transform=\"translate(500,0)\" style=\"stroke-width: 3;\"></path><path data-c=\"74\" d=\"M27 422Q80 426 109 478T141 600V615H181V431H316V385H181V241Q182 116 182 100T189 68Q203 29 238 29Q282 29 292 100Q293 108 293 146V181H333V146V134Q333 57 291 17Q264 -10 221 -10Q187 -10 162 2T124 33T105 68T98 100Q97 107 97 248V385H18V422H27Z\" transform=\"translate(889,0)\" style=\"stroke-width: 3;\"></path><path data-c=\"6E\" d=\"M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q450 438 463 329Q464 322 464 190V104Q464 66 466 59T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z\" transform=\"translate(1278,0)\" style=\"stroke-width: 3;\"></path></g></g></g></g></svg><mjx-assistive-mml unselectable=\"on\" display=\"inline\" style=\"top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>f</mi><mtext>attn</mtext></msub></math></mjx-assistive-mml></mjx-container>, we can see that <mjx-container class=\"MathJax\" jax=\"SVG\" style=\"direction: ltr; position: relative;\"><svg style=\"overflow: visible; min-height: 1px; min-width: 1px; vertical-align: -0.566ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"24.962ex\" height=\"2.262ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -750 11033.2 1000\" aria-hidden=\"true\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"msub\"><g data-mml-node=\"mi\"><path data-c=\"1D453\" d=\"M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z\" style=\"stroke-width: 3;\"></path></g><g data-mml-node=\"mtext\" transform=\"translate(523,-150) scale(0.707)\"><path data-c=\"61\" d=\"M137 305T115 305T78 320T63 359Q63 394 97 421T218 448Q291 448 336 416T396 340Q401 326 401 309T402 194V124Q402 76 407 58T428 40Q443 40 448 56T453 109V145H493V106Q492 66 490 59Q481 29 455 12T400 -6T353 12T329 54V58L327 55Q325 52 322 49T314 40T302 29T287 17T269 6T247 -2T221 -8T190 -11Q130 -11 82 20T34 107Q34 128 41 147T68 188T116 225T194 253T304 268H318V290Q318 324 312 340Q290 411 215 411Q197 411 181 410T156 406T148 403Q170 388 170 359Q170 334 154 320ZM126 106Q126 75 150 51T209 26Q247 26 276 49T315 109Q317 116 318 175Q318 233 317 233Q309 233 296 232T251 223T193 203T147 166T126 106Z\" style=\"stroke-width: 3;\"></path><path data-c=\"74\" d=\"M27 422Q80 426 109 478T141 600V615H181V431H316V385H181V241Q182 116 182 100T189 68Q203 29 238 29Q282 29 292 100Q293 108 293 146V181H333V146V134Q333 57 291 17Q264 -10 221 -10Q187 -10 162 2T124 33T105 68T98 100Q97 107 97 248V385H18V422H27Z\" transform=\"translate(500,0)\" style=\"stroke-width: 3;\"></path><path data-c=\"74\" d=\"M27 422Q80 426 109 478T141 600V615H181V431H316V385H181V241Q182 116 182 100T189 68Q203 29 238 29Q282 29 292 100Q293 108 293 146V181H333V146V134Q333 57 291 17Q264 -10 221 -10Q187 -10 162 2T124 33T105 68T98 100Q97 107 97 248V385H18V422H27Z\" transform=\"translate(889,0)\" style=\"stroke-width: 3;\"></path><path data-c=\"6E\" d=\"M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q450 438 463 329Q464 322 464 190V104Q464 66 466 59T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z\" transform=\"translate(1278,0)\" style=\"stroke-width: 3;\"></path></g></g><g data-mml-node=\"mo\" transform=\"translate(1869.8,0)\"><path data-c=\"28\" d=\"M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z\" style=\"stroke-width: 3;\"></path></g><g data-mml-node=\"mi\" transform=\"translate(2258.8,0)\"><path data-c=\"1D454\" d=\"M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z\" style=\"stroke-width: 3;\"></path></g><g data-mml-node=\"mo\" transform=\"translate(2735.8,0)\"><path data-c=\"28\" d=\"M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z\" style=\"stroke-width: 3;\"></path></g><g data-mml-node=\"mi\" transform=\"translate(3124.8,0)\"><path data-c=\"1D47F\" d=\"M931 686Q953 686 953 670Q953 650 944 632Q936 624 924 624H914Q823 624 803 611Q800 609 696 503T591 396Q591 394 667 229L743 62H787H814Q846 62 846 44Q843 7 829 2Q825 0 817 0Q813 0 775 1T664 2Q590 2 551 1T508 0H507Q484 0 484 18Q484 19 488 37Q492 56 497 58T534 62L566 63Q567 64 520 169T471 274Q469 274 369 172T268 67L315 62Q320 62 328 62L335 61Q347 58 347 44Q344 10 331 2L326 0L287 1Q263 2 177 2Q95 2 78 1L53 0Q38 6 38 17Q38 40 50 57Q56 62 78 62Q169 62 188 75Q194 77 435 324L444 334L439 347Q437 351 373 492L313 624H268H246Q220 624 212 632Q210 636 210 642Q210 655 215 669T227 684Q230 686 247 686Q295 684 398 684Q438 684 472 684T527 685T551 686Q567 686 572 671Q572 667 568 651Q563 631 558 628T523 624T492 623H488L526 540Q563 457 564 457Q564 456 574 466T604 496T645 537L724 619Q716 622 677 624H673Q645 624 645 640Q645 660 654 678Q659 683 666 686L704 685Q728 684 813 684Q847 684 873 684T913 685T931 686Z\" style=\"stroke-width: 3;\"></path></g><g data-mml-node=\"mo\" transform=\"translate(4071.8,0)\"><path data-c=\"29\" d=\"M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z\" style=\"stroke-width: 3;\"></path></g><g data-mml-node=\"mo\" transform=\"translate(4460.8,0)\"><path data-c=\"29\" d=\"M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z\" style=\"stroke-width: 3;\"></path></g><g data-mml-node=\"mo\" transform=\"translate(5127.6,0)\"><path data-c=\"3D\" d=\"M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z\" style=\"stroke-width: 3;\"></path></g><g data-mml-node=\"mi\" transform=\"translate(6183.4,0)\"><path data-c=\"1D454\" d=\"M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z\" style=\"stroke-width: 3;\"></path></g><g data-mml-node=\"mo\" transform=\"translate(6660.4,0)\"><path data-c=\"28\" d=\"M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z\" style=\"stroke-width: 3;\"></path></g><g data-mml-node=\"msub\" transform=\"translate(7049.4,0)\"><g data-mml-node=\"mi\"><path data-c=\"1D453\" d=\"M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z\" style=\"stroke-width: 3;\"></path></g><g data-mml-node=\"mtext\" transform=\"translate(523,-150) scale(0.707)\"><path data-c=\"61\" d=\"M137 305T115 305T78 320T63 359Q63 394 97 421T218 448Q291 448 336 416T396 340Q401 326 401 309T402 194V124Q402 76 407 58T428 40Q443 40 448 56T453 109V145H493V106Q492 66 490 59Q481 29 455 12T400 -6T353 12T329 54V58L327 55Q325 52 322 49T314 40T302 29T287 17T269 6T247 -2T221 -8T190 -11Q130 -11 82 20T34 107Q34 128 41 147T68 188T116 225T194 253T304 268H318V290Q318 324 312 340Q290 411 215 411Q197 411 181 410T156 406T148 403Q170 388 170 359Q170 334 154 320ZM126 106Q126 75 150 51T209 26Q247 26 276 49T315 109Q317 116 318 175Q318 233 317 233Q309 233 296 232T251 223T193 203T147 166T126 106Z\" style=\"stroke-width: 3;\"></path><path data-c=\"74\" d=\"M27 422Q80 426 109 478T141 600V615H181V431H316V385H181V241Q182 116 182 100T189 68Q203 29 238 29Q282 29 292 100Q293 108 293 146V181H333V146V134Q333 57 291 17Q264 -10 221 -10Q187 -10 162 2T124 33T105 68T98 100Q97 107 97 248V385H18V422H27Z\" transform=\"translate(500,0)\" style=\"stroke-width: 3;\"></path><path data-c=\"74\" d=\"M27 422Q80 426 109 478T141 600V615H181V431H316V385H181V241Q182 116 182 100T189 68Q203 29 238 29Q282 29 292 100Q293 108 293 146V181H333V146V134Q333 57 291 17Q264 -10 221 -10Q187 -10 162 2T124 33T105 68T98 100Q97 107 97 248V385H18V422H27Z\" transform=\"translate(889,0)\" style=\"stroke-width: 3;\"></path><path data-c=\"6E\" d=\"M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q450 438 463 329Q464 322 464 190V104Q464 66 466 59T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z\" transform=\"translate(1278,0)\" style=\"stroke-width: 3;\"></path></g></g><g data-mml-node=\"mo\" transform=\"translate(8919.2,0)\"><path data-c=\"28\" d=\"M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z\" style=\"stroke-width: 3;\"></path></g><g data-mml-node=\"mi\" transform=\"translate(9308.2,0)\"><path data-c=\"1D47F\" d=\"M931 686Q953 686 953 670Q953 650 944 632Q936 624 924 624H914Q823 624 803 611Q800 609 696 503T591 396Q591 394 667 229L743 62H787H814Q846 62 846 44Q843 7 829 2Q825 0 817 0Q813 0 775 1T664 2Q590 2 551 1T508 0H507Q484 0 484 18Q484 19 488 37Q492 56 497 58T534 62L566 63Q567 64 520 169T471 274Q469 274 369 172T268 67L315 62Q320 62 328 62L335 61Q347 58 347 44Q344 10 331 2L326 0L287 1Q263 2 177 2Q95 2 78 1L53 0Q38 6 38 17Q38 40 50 57Q56 62 78 62Q169 62 188 75Q194 77 435 324L444 334L439 347Q437 351 373 492L313 624H268H246Q220 624 212 632Q210 636 210 642Q210 655 215 669T227 684Q230 686 247 686Q295 684 398 684Q438 684 472 684T527 685T551 686Q567 686 572 671Q572 667 568 651Q563 631 558 628T523 624T492 623H488L526 540Q563 457 564 457Q564 456 574 466T604 496T645 537L724 619Q716 622 677 624H673Q645 624 645 640Q645 660 654 678Q659 683 666 686L704 685Q728 684 813 684Q847 684 873 684T913 685T931 686Z\" style=\"stroke-width: 3;\"></path></g><g data-mml-node=\"mo\" transform=\"translate(10255.2,0)\"><path data-c=\"29\" d=\"M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z\" style=\"stroke-width: 3;\"></path></g><g data-mml-node=\"mo\" transform=\"translate(10644.2,0)\"><path data-c=\"29\" d=\"M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z\" style=\"stroke-width: 3;\"></path></g></g></g></svg><mjx-assistive-mml unselectable=\"on\" display=\"inline\" style=\"top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>f</mi><mtext>attn</mtext></msub><mo stretchy=\"false\">(</mo><mi>g</mi><mo stretchy=\"false\">(</mo><mi mathvariant=\"bold-italic\">X</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo><mo>=</mo><mi>g</mi><mo stretchy=\"false\">(</mo><msub><mi>f</mi><mtext>attn</mtext></msub><mo stretchy=\"false\">(</mo><mi mathvariant=\"bold-italic\">X</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo></math></mjx-assistive-mml></mjx-container>, for any shuffling operation <mjx-container class=\"MathJax\" jax=\"SVG\" style=\"direction: ltr; position: relative;\"><svg style=\"overflow: visible; min-height: 1px; min-width: 1px; vertical-align: -0.464ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"1.079ex\" height=\"1.464ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -442 477 647\" aria-hidden=\"true\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"mi\"><path data-c=\"1D454\" d=\"M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z\" style=\"stroke-width: 3;\"></path></g></g></g></svg><mjx-assistive-mml unselectable=\"on\" display=\"inline\" style=\"top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>g</mi></math></mjx-assistive-mml></mjx-container>.</p>\n<p>Permutation equivariance is useful for any kind of data modality where the inputs are not really a sequence, but can rather be described as a set (i.e. ordering does not matter).\nIn language, however, it does, which is why transformers were originally proposed along with positional encodings.</p>\n<h2 id=\"positional-embeddings-break-permutation-equivariance\" tabindex=\"-1\">Positional embeddings break permutation equivariance <a class=\"header-anchor\" href=\"https://gdewael.github.io/blog/equivariance/\">#</a></h2>\n<p>Positional encodings are most-simply introduced by adding a position-dependent signal to the input.\nFor example:</p>\n<div style=\"text-align: center;\" id=\"vis-pos\"></div>\n<p>You will see that permutation equivariance is broken by the positional encodings.\nI.e., a shuffled input will not return the same output - albeit shuffled the same way - anymore.\nBy communicating positional indices, we do not operate on an unordered set.\nRather, the model becomes a true sequence model.</p>\n<p>Note that this example features positional encodings that simply contain the positional indices.\nIn practice, positional encodings may be sinusoidal (which has a nice decaying similarity effect on the dot product attention matrix), as in the original transformer publication<sup class=\"footnote-ref\"><a href=\"https://gdewael.github.io/blog/equivariance/\" id=\"fnref3\">[3]</a></sup>.\nGiven enough data, one may also choose to learn the positional embeddings from scratch, as in the BERT model<sup class=\"footnote-ref\"><a href=\"https://gdewael.github.io/blog/equivariance/\" id=\"fnref4\">[4]</a></sup>.</p>\n<h2 id=\"time-shift-translation-equivariance-through-relative-positional-encodings\" tabindex=\"-1\">Time-shift (translation) equivariance through relative positional encodings <a class=\"header-anchor\" href=\"https://gdewael.github.io/blog/equivariance/\">#</a></h2>\n<p>In many data domains, the absolute positioning of elements in the sequence does not matter.\nIn these domains, how signals co-occur relative to eachother may be more relevant.\nFor example, in images, the absolute location of a cat's ear is inconsequential to its detection.\nRather, the fact that a cat's ear should be located on top of its head is a relevant signal.\nConvolutions have this built-in, as they are translation equivariant: given a shift in pixels, a convolution will return the same activation map, albeit shifted by the same amount.\nIn language, this might also be an attractive feature.\nConsider that the triplet of words &quot;Love conquers all&quot;, may occur anywhere within a larger paragraph of text:</p>\n<div id=\"paragraph\"></div>\n<p>Irregardless of its location within a paragraph, &quot;Love&quot; will always be the grammatical subject of the clause, and &quot;conquers&quot; its verb.\nHow the words interact within the three word clause remains the same, no matter where in the paragraph it appears.\nA beneficial property of a language model could, hence, be, to be robust against translations or time-shifts in words.\nOne can build such <em>translation equivariance</em> - or in the case of sequence models, also called time-shift equivariance - into transformer models using relative positional encodings.\nOne example of such a relative positional encoding scheme is rotary embeddings<sup class=\"footnote-ref\"><a href=\"https://gdewael.github.io/blog/equivariance/\" id=\"fnref5\">[5]</a></sup>, which are applied in favor of absolute encodings in many of the recent LLMs.</p>\n<p>To visualize rotary embeddings in action, let us add a slider to the previous example that lets you control where you place the words:</p>\n<style>\n    .row {\n    display: flex;\n    clear: both;\n    }\n\n    .column {\n    float: left;\n    padding: 10px;\n    }\n\n    .left {\n    width: 35%;\n    }\n\n    .right {\n    width: 65%;\n    }\n    input,output{display: inline-block;\n    vertical-align: middle;}\n</style>\n<div class=\"row\">\n  <div class=\"column left\">\n    <br>\n    Sentence position: <br>\n    <input type=\"range\" name=\"slider\" id=\"slider\" min=\"0\" max=\"35\" value=\"0\" oninput=\"slideroutput.value = slider.value.toString()+&quot; - &quot;+(+slider.value+2).toString()\">  <output id=\"slideroutput\">0 - 2</output>\n  </div>\n  <div class=\"column right\"><div id=\"paragraph2\"></div></div>\n</div> \n<p>Embedding the three-word clause in the same way:</p>\n<div style=\"text-align: center;\" id=\"vis-x-rel\"></div>\n<div style=\"text-align: center;\" id=\"vis-qkv-rel\"></div>\n<p>With rotary embeddings, the queries and key matrices are rotated according to their position index:</p>\n<div style=\"text-align: center;\" id=\"vis-rot-rel\"></div>\n<p>Using the same attention operations:</p>\n<div style=\"text-align: center;\" id=\"vis-attn-rel\"></div>\n<p>One sees that dot products of index-rotated queries and keys are preserved when said index changes.\nThis, in turn, gives us a translation-equivariant self-attention mechanism.</p>\n<p>Note that in this example, the visualized outputs do not change if the inputs are shifted, suggesting <em>invariance</em> rather than equivariance.\nIt is important to keep in mind that this visualization only shows the three example tokens in the larger sequence.\nIn the broader context of the paragraph, the embeddings of these three example tokens would be similarly shifted according to its indices.</p>\n<h2 id=\"exploiting-other-symmetries\" tabindex=\"-1\">Exploiting other symmetries <a class=\"header-anchor\" href=\"https://gdewael.github.io/blog/equivariance/\">#</a></h2>\n<p>Recently, <em>SE(3)-equivariant</em> self-attention variants have been described<sup class=\"footnote-ref\"><a href=\"https://gdewael.github.io/blog/equivariance/\" id=\"fnref6\">[6]</a></sup>.\nThese operations are agnostic to rotations and translations of inputs.\nIt's a useful property to have when operating on 3D coordinates as inputs.\nFor example, for an input molecule, a neural network should deliver the same output if said molecule is inputted with slightly different coordinates for its atoms.\nAn interactive D3.js visualization of this mechanism is for a next post.</p>\n<h2 id=\"addendum-what-about-invariance\" tabindex=\"-1\">Addendum: What about <em>invariance</em>? <a class=\"header-anchor\" href=\"https://gdewael.github.io/blog/equivariance/\">#</a></h2>\n<p>Equivariances are nice to have for your model layers internally.\nIn the end, however, the final representation is still dependent on the original ordering.\nImagine concatenating all token representations across a sequence, and linearly projecting those to make final a final prediction.\nIn that case, different orderings of data will still result in different predictions.\nWhat we want at the end of the model is, hence, often <em>invariance</em>.\nA simple way to achieve this with a transformer is either through pre-pending a classification (<mjx-container class=\"MathJax\" jax=\"SVG\" style=\"direction: ltr; position: relative;\"><svg style=\"overflow: visible; min-height: 1px; min-width: 1px; vertical-align: -0.186ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"5.939ex\" height=\"1.756ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -694 2625 776\" aria-hidden=\"true\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"mtext\"><path data-c=\"5B\" d=\"M237 -82Q221 -78 214 -58V305Q214 669 216 673Q220 687 231 690T278 694H350H461Q462 693 467 690T474 685T478 679T482 670T483 656Q483 632 471 625T428 617Q422 617 406 617T379 618H298V-7H379H420Q459 -7 471 -13T483 -45Q483 -55 483 -59T477 -70T461 -82H237Z\" style=\"stroke-width: 3;\"></path><path data-c=\"1D672\" d=\"M40 305Q40 437 110 529T281 622Q315 622 343 611T387 589T404 578Q409 585 415 596T425 611T435 618T452 622Q472 622 478 609T485 566Q485 559 485 540T484 508V460Q484 413 478 403T442 393Q417 393 409 402Q400 409 400 420Q400 428 395 445T380 487T347 528T295 546Q235 546 180 483T124 306Q124 245 141 197T186 121T241 80T296 66Q346 66 373 103T400 178Q400 209 435 209H442H450Q484 209 484 172Q480 96 421 43T281 -11Q177 -11 109 84T40 305Z\" transform=\"translate(525,0)\" style=\"stroke-width: 3;\"></path><path data-c=\"1D67B\" d=\"M27 594Q34 605 43 608T84 611H154H213Q258 611 269 605T281 573Q281 546 263 538Q257 535 222 535H185V76H404V118V145Q404 168 411 177T446 186H453Q478 186 486 167Q488 161 488 93V50Q488 24 485 17T466 1L258 0H147H99Q47 0 36 6T25 38Q25 59 35 69Q44 76 76 76H101V535H76H64Q36 535 27 552Q25 557 25 573T27 594Z\" transform=\"translate(1050,0)\" style=\"stroke-width: 3;\"></path><path data-c=\"1D682\" d=\"M52 454Q52 524 107 572T229 621Q266 621 274 620Q326 610 360 588L371 581Q377 594 379 598T386 610T397 619T412 622Q433 622 439 610T446 570Q446 563 446 545T445 515V479Q445 441 444 432T436 417Q428 408 403 408T370 417Q361 424 361 434Q361 439 360 448T351 476T331 509T295 535T238 546Q194 546 163 522T132 458Q132 435 148 412Q155 401 166 393T192 380T218 371T247 364T270 359Q341 342 349 339Q389 325 418 296T461 229Q472 201 472 164Q469 92 417 41T287 -11Q240 -11 200 -1T143 19L126 29Q117 6 109 -2Q100 -11 84 -11Q64 -11 58 1T51 42Q51 49 51 66T52 95V135Q52 173 53 180T61 194Q70 203 95 203Q119 203 127 194Q136 186 136 168Q143 66 284 66H290Q325 66 350 85Q391 115 391 165Q391 204 369 228T322 260Q320 260 255 275T185 293Q123 309 88 355T52 454Z\" transform=\"translate(1575,0)\" style=\"stroke-width: 3;\"></path><path data-c=\"5D\" d=\"M41 656Q41 681 53 688T99 695Q107 695 133 695T177 694H288Q307 681 310 669V-58Q303 -76 288 -82H64Q41 -73 41 -45Q41 -21 53 -14T96 -6Q102 -6 118 -6T145 -7H226V618H145H100Q67 618 54 625T41 656Z\" transform=\"translate(2100,0)\" style=\"stroke-width: 3;\"></path></g></g></g></svg><mjx-assistive-mml unselectable=\"on\" display=\"inline\" style=\"top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mtext mathvariant=\"monospace\">[CLS]</mtext></math></mjx-assistive-mml></mjx-container>) token to the input, which is first in the sequence no matter what.\nThe output embedding at the first index will then be permutation invariant if all internal layers were permutation equivariant.\nAnother - admittedly simpler - way is globally max-pooling across all sequence tokens.</p>\n<script type=\"module\">\n\nimport * as d3 from \"https://cdn.jsdelivr.net/npm/d3@7/+esm\";\n\nfunction fillmatrix(matrix, coords, label, to) {\n    coords.forEach(i => d3.select(to).selectAll('tspan[*|label=\"'+label+i+'\"]').text(matrix[i[0]-1][i[1]-1]));\n};\n\nfunction fillmatrix_formatted(matrix, coords, label, format, to) {\n    const f = d3.format(format);\n    coords.forEach(i => d3.select(to).selectAll('tspan[*|label=\"'+label+i+'\"]').text(f(matrix[i[0]-1][i[1]-1])));\n};\n\nfunction multiplyMatrices(m1, m2) {\n    var result = [];\n    for (var i = 0; i < m1.length; i++) {\n        result[i] = [];\n        for (var j = 0; j < m2[0].length; j++) {\n            var sum = 0;\n            for (var k = 0; k < m1[0].length; k++) {\n                sum += m1[i][k] * m2[k][j];\n            }\n            result[i][j] = sum;\n        }\n    }\n    return result;\n};\n\nfunction transpose(matrix) {\n    return matrix[0].map((col, i) => matrix.map(row => row[i]));\n};\n\nfunction softmax(arr) {\n    return arr.map(function(value,index) { \n        return Math.exp(value) / arr.map( function(y /*value*/){ return Math.exp(y) } ).reduce( function(a,b){ return a+b });\n    });\n};\n\nfunction compute(X, P, Wq, Wk, Wv, add_P=false) {\n    if (add_P) {\n        X = [[X[0][0] + P[0][0], X[0][1] + P[0][1]], [X[1][0] + P[1][0], X[1][1] + P[1][1]], [X[2][0] + P[2][0], X[2][1] + P[2][1]]];\n    }\n    var Q = multiplyMatrices(X, Wq);\n    var K = multiplyMatrices(X, Wk);\n    var V = multiplyMatrices(X, Wv);\n    var QK = multiplyMatrices(Q, transpose(K));\n    var A = [softmax(QK[0].map(i => i / 1.41421)), softmax(QK[1].map(i => i / 1.41421)), softmax(QK[2].map(i => i / 1.41421))];\n    var Z = multiplyMatrices(A, V);\n    return [Q,K,V,QK,A,Z];\n};\n\nfunction computeRotary(X, P, Wq, Wk, Wv, start_index) {\n    var Q = multiplyMatrices(X, Wq);\n    var K = multiplyMatrices(X, Wk);\n    var V = multiplyMatrices(X, Wv);\n\n    const pos = [start_index, start_index+1, start_index+2];\n\n    var get_rotmat = function(p) {\n        return [[Math.cos(p), -Math.sin(p)], [Math.sin(p), Math.cos(p)]];\n    };\n    const rots = pos.map(get_rotmat);\n    const Qrot = [\n        multiplyMatrices(rots[0], transpose([Q[0]])),\n        multiplyMatrices(rots[1], transpose([Q[1]])),\n        multiplyMatrices(rots[2], transpose([Q[2]])),\n    ];\n    const Krot = [\n        multiplyMatrices(rots[0], transpose([K[0]])),\n        multiplyMatrices(rots[1], transpose([K[1]])),\n        multiplyMatrices(rots[2], transpose([K[2]])),\n    ];\n\n    var QK = multiplyMatrices(Qrot, transpose(Krot));\n    var A = [softmax(QK[0].map(i => i / 1.41421)), softmax(QK[1].map(i => i / 1.41421)), softmax(QK[2].map(i => i / 1.41421))];\n    var Z = multiplyMatrices(A, V);\n    return [Q,K,V,Qrot,Krot,QK,A,Z];\n};\n\nfunction shuffleArray(array) {\n    let currentIndex = array.length;\n\n    // While there remain elements to shuffle...\n    while (currentIndex != 0) {\n\n        // Pick a remaining element...\n        let randomIndex = Math.floor(Math.random() * currentIndex);\n        currentIndex--;\n\n        // And swap it with the current element.\n        [array[currentIndex], array[randomIndex]] = [\n        array[randomIndex], array[currentIndex]];\n    }\n    };\n\nfunction AttnHover(to) {\n    function preSoftHover(item, to) {\n        const code = item[0];\n        const orig_color = d3.select(to).selectAll('rect[*|label*=\"presoft_'+item[1]+'\"]').style(\"fill\");\n\n        const selection = d3.select(to).selectAll('g[*|label*=\"presoft_'+code+'\"]');\n        selection.on('mouseover.'+code, function(d) {\n            d3.select(this).selectChildren(\"rect\").style(\"fill\", \"#a1c9f4\");\n            d3.select(to).selectAll('rect[*|label*=\"presoft_'+item[1]+'\"]').style(\"fill\", \"#a1c9f4\").style(\"opacity\", 1.0);\n        });\n        selection.on('mouseout.'+code, function(d) {\n            d3.select(this).selectChildren(\"rect\").style(\"fill\", \"white\");\n            d3.select(to).selectAll('rect[*|label*=\"presoft_'+item[1]+'\"]').style(\"fill\", orig_color).style(\"opacity\", 0.33);\n        });\n    };\n\n\n    [[\"c1\", \"k1\"], [\"c2\", \"k2\"], [\"c3\", \"k3\"]].forEach(i => preSoftHover(i, to));\n    [[\"r1\", \"q1\"], [\"r2\", \"q2\"], [\"r3\", \"q3\"]].forEach(i => preSoftHover(i, to));\n\n    const z_r1 = d3.select(to).selectAll('g[*|label*=\"z_r1\"]');\n    var z_r1_color = z_r1.selectChildren(\"rect\").style(\"fill\");\n    z_r1.on(\"mouseover.z_r1\", function(d) {\n        d3.select(this).selectChildren(\"rect\").style(\"fill\", \"#a1c9f4\").style(\"opacity\", 1.0);\n        d3.select(to).selectAll('g[*|label*=\"A1\"]').selectChildren(\"rect\").style(\"fill\", \"#a1c9f4\");\n    });\n    z_r1.on(\"mouseout.z_r1\", function(d) {\n        d3.select(this).selectChildren(\"rect\").style(\"fill\", z_r1_color).style(\"opacity\", 0.33);\n        d3.select(to).selectAll('g[*|label*=\"A1\"]').selectChildren(\"rect\").style(\"fill\", \"white\");\n    });\n\n    const z_r2 = d3.select(to).selectAll('g[*|label*=\"z_r2\"]');\n    var z_r2_color = z_r2.selectChildren(\"rect\").style(\"fill\");\n    z_r2.on(\"mouseover.z_r2\", function(d) {\n        d3.select(this).selectChildren(\"rect\").style(\"fill\", \"#a1c9f4\").style(\"opacity\", 1.0);\n        d3.select(to).selectAll('g[*|label*=\"A2\"]').selectChildren(\"rect\").style(\"fill\", \"#a1c9f4\");\n    });\n    z_r2.on(\"mouseout.z_r2\", function(d) {\n        d3.select(this).selectChildren(\"rect\").style(\"fill\", z_r2_color).style(\"opacity\", 0.33);\n        d3.select(to).selectAll('g[*|label*=\"A2\"]').selectChildren(\"rect\").style(\"fill\", \"white\");\n    });\n\n    const z_r3 = d3.select(to).selectAll('g[*|label*=\"z_r3\"]');\n    var z_r3_color = z_r3.selectChildren(\"rect\").style(\"fill\");\n    z_r3.on(\"mouseover.z_r3\", function(d) {\n        d3.select(this).selectChildren(\"rect\").style(\"fill\", \"#a1c9f4\").style(\"opacity\", 1.0);\n        d3.select(to).selectAll('g[*|label*=\"A3\"]').selectChildren(\"rect\").style(\"fill\", \"#a1c9f4\");\n    });\n    z_r3.on(\"mouseout.z_r3\", function(d) {\n        d3.select(this).selectChildren(\"rect\").style(\"fill\", z_r3_color).style(\"opacity\", 0.33);\n        d3.select(to).selectAll('g[*|label*=\"A3\"]').selectChildren(\"rect\").style(\"fill\", \"white\");\n    });\n\n    var v_c1_color = d3.select(to).selectAll('g[*|label*=\"v_r1\"]').selectChildren(\"rect\").style(\"fill\");\n    var v_c2_color = d3.select(to).selectAll('g[*|label*=\"v_r2\"]').selectChildren(\"rect\").style(\"fill\");\n    var v_c3_color = d3.select(to).selectAll('g[*|label*=\"v_r3\"]').selectChildren(\"rect\").style(\"fill\");\n\n    const z_c1 = d3.select(to).selectAll('g[*|label*=\"z_c1\"]');\n    z_c1.on(\"mouseover.z_c1\", function(d) {\n        d3.select(this).selectChildren(\"rect\").style(\"fill\", \"#a1c9f4\");\n        d3.select(to).selectAll('g[*|label*=\"v_c1\"]').selectChildren(\"rect\").style(\"fill\", \"#a1c9f4\").style(\"opacity\", 1.0);\n    });\n    z_c1.on(\"mouseout.z_c1\", function(d) {\n        d3.select(to).selectAll('g[*|label*=\"v_r1_v_c1\"]').selectChildren(\"rect\").style(\"fill\", v_c1_color).style(\"opacity\", 0.33);\n        d3.select(to).selectAll('g[*|label*=\"v_r2_v_c1\"]').selectChildren(\"rect\").style(\"fill\", v_c2_color).style(\"opacity\", 0.33);\n        d3.select(to).selectAll('g[*|label*=\"v_r3_v_c1\"]').selectChildren(\"rect\").style(\"fill\", v_c3_color).style(\"opacity\", 0.33);\n    });\n\n    const z_c2 = d3.select(to).selectAll('g[*|label*=\"z_c2\"]');\n    z_c2.on(\"mouseover.z_c2\", function(d) {\n        d3.select(this).selectChildren(\"rect\").style(\"fill\", \"#a1c9f4\");\n        d3.select(to).selectAll('g[*|label*=\"v_c2\"]').selectChildren(\"rect\").style(\"fill\", \"#a1c9f4\").style(\"opacity\", 1.0);\n    });\n    z_c2.on(\"mouseout.z_c2\", function(d) {\n        d3.select(to).selectAll('g[*|label*=\"v_r1_v_c2\"]').selectChildren(\"rect\").style(\"fill\", v_c1_color).style(\"opacity\", 0.33);\n        d3.select(to).selectAll('g[*|label*=\"v_r2_v_c2\"]').selectChildren(\"rect\").style(\"fill\", v_c2_color).style(\"opacity\", 0.33);\n        d3.select(to).selectAll('g[*|label*=\"v_r3_v_c2\"]').selectChildren(\"rect\").style(\"fill\", v_c3_color).style(\"opacity\", 0.33);\n    });\n    \n};\n\nfunction drawX(X, words, order, from, to, color=false) {\n    d3.xml(from)\n    .then(data => {\n        if (d3.select(to).node().children.length == 0) {\n            d3.select(to).node().append(data.documentElement);\n        }\n\n        let words_new = [\n            words[0].charAt(0).toUpperCase() + words[0].slice(1),\n            words[1],\n            words[2],\n        ];\n\n        [0,1,2].forEach(i => d3.select(to).selectAll('tspan[*|label=\"word'+(i+1)+'\"]').text(words_new[i]));\n        fillmatrix(X, [\"11\", \"12\", \"21\", \"22\", \"31\", \"32\"], \"x\", to);\n        d3.select(to).selectAll('text').style(\"cursor\", \"default\");\n        \n        if (color) {\n            var colors = [\"#ffb482\", \"#8de5a1\", \"#ff9f9b\"];\n            d3.select(to).selectAll('rect[*|label*=\"X1\"]').style(\"fill\", colors[order[0]]).style(\"opacity\", 0.33);\n            d3.select(to).selectAll('rect[*|label*=\"X2\"]').style(\"fill\", colors[order[1]]).style(\"opacity\", 0.33);\n            d3.select(to).selectAll('rect[*|label*=\"X3\"]').style(\"fill\", colors[order[2]]).style(\"opacity\", 0.33);\n        };\n        });\n};\n\nfunction drawQKV(Q,K,V, Wq, Wk, Wv, order, from, to, color=false) {\n    d3.xml(from)\n    .then(data => {\n        if (d3.select(to).node().children.length == 0) {\n            d3.select(to).node().append(data.documentElement);\n        }\n\n        d3.select(to).selectAll('text').style(\"cursor\", \"default\");\n\n        fillmatrix(Q, [\"11\", \"12\", \"21\", \"22\", \"31\", \"32\"], \"Q\", to);\n        fillmatrix(K, [\"11\", \"12\", \"21\", \"22\", \"31\", \"32\"], \"K\", to);\n        fillmatrix(V, [\"11\", \"12\", \"21\", \"22\", \"31\", \"32\"], \"V\", to);\n        fillmatrix(Wq, [\"11\", \"12\", \"21\", \"22\"], \"Wq\", to);\n        fillmatrix(Wk, [\"11\", \"12\", \"21\", \"22\"], \"Wk\", to);\n        fillmatrix(Wv, [\"11\", \"12\", \"21\", \"22\"], \"Wv\", to);\n\n        if (color) {\n            var colors = [\"#ffb482\", \"#8de5a1\", \"#ff9f9b\"];\n            d3.select(to).selectAll('rect[*|label*=\"Q1\"]').style(\"fill\", colors[order[0]]).style(\"opacity\", 0.33);\n            d3.select(to).selectAll('rect[*|label*=\"Q2\"]').style(\"fill\", colors[order[1]]).style(\"opacity\", 0.33);\n            d3.select(to).selectAll('rect[*|label*=\"Q3\"]').style(\"fill\", colors[order[2]]).style(\"opacity\", 0.33);\n            d3.select(to).selectAll('rect[*|label*=\"K1\"]').style(\"fill\", colors[order[0]]).style(\"opacity\", 0.33);\n            d3.select(to).selectAll('rect[*|label*=\"K2\"]').style(\"fill\", colors[order[1]]).style(\"opacity\", 0.33);\n            d3.select(to).selectAll('rect[*|label*=\"K3\"]').style(\"fill\", colors[order[2]]).style(\"opacity\", 0.33);\n            d3.select(to).selectAll('rect[*|label*=\"V1\"]').style(\"fill\", colors[order[0]]).style(\"opacity\", 0.33);\n            d3.select(to).selectAll('rect[*|label*=\"V2\"]').style(\"fill\", colors[order[1]]).style(\"opacity\", 0.33);\n            d3.select(to).selectAll('rect[*|label*=\"V3\"]').style(\"fill\", colors[order[2]]).style(\"opacity\", 0.33);\n        };\n        });\n};\n\nfunction drawAttn(Q,K,V,QK,A,Z, order, from, to, color=false, format_QK=false) {\n    d3.xml(from)\n    .then(data => {\n        if (d3.select(to).node().children.length == 0) {\n            d3.select(to).node().append(data.documentElement);\n        }\n\n        d3.select(to).selectAll('text').style(\"cursor\", \"default\");\n\n        if (format_QK) {\n            fillmatrix_formatted(Q, [\"11\", \"12\", \"21\", \"22\", \"31\", \"32\"], \"Q\", \".2f\", to);\n            fillmatrix_formatted(K, [\"11\", \"12\", \"21\", \"22\", \"31\", \"32\"], \"K\", \".2f\", to);\n            fillmatrix_formatted(V, [\"11\", \"12\", \"21\", \"22\", \"31\", \"32\"], \"V\", \".2f\", to);\n            fillmatrix_formatted(QK, [\"11\", \"12\", \"13\", \"21\", \"22\", \"23\", \"31\", \"32\", \"33\"], \"QK\", \".2f\", to);\n        } else {\n            fillmatrix(Q, [\"11\", \"12\", \"21\", \"22\", \"31\", \"32\"], \"Q\", to);\n            fillmatrix(K, [\"11\", \"12\", \"21\", \"22\", \"31\", \"32\"], \"K\", to);\n            fillmatrix(V, [\"11\", \"12\", \"21\", \"22\", \"31\", \"32\"], \"V\", to);\n            fillmatrix(QK, [\"11\", \"12\", \"13\", \"21\", \"22\", \"23\", \"31\", \"32\", \"33\"], \"QK\", to);\n        }\n        \n        \n        fillmatrix_formatted(A, [\"11\", \"12\", \"13\", \"21\", \"22\", \"23\", \"31\", \"32\", \"33\"], \"A\", \".2f\", to);\n        fillmatrix_formatted(Z, [\"11\", \"12\", \"21\", \"22\", \"31\", \"32\"], \"Z\", \".2f\", to);\n\n        if (color) {\n            var colors = [\"#ffb482\", \"#8de5a1\", \"#ff9f9b\"];\n            d3.select(to).selectAll('rect[*|label*=\"presoft_k1\"]').style(\"fill\", colors[order[0]]).style(\"opacity\", 0.33);\n            d3.select(to).selectAll('rect[*|label*=\"presoft_k2\"]').style(\"fill\", colors[order[1]]).style(\"opacity\", 0.33);\n            d3.select(to).selectAll('rect[*|label*=\"presoft_k3\"]').style(\"fill\", colors[order[2]]).style(\"opacity\", 0.33);\n            d3.select(to).selectAll('rect[*|label*=\"presoft_q1\"]').style(\"fill\", colors[order[0]]).style(\"opacity\", 0.33);\n            d3.select(to).selectAll('rect[*|label*=\"presoft_q2\"]').style(\"fill\", colors[order[1]]).style(\"opacity\", 0.33);\n            d3.select(to).selectAll('rect[*|label*=\"presoft_q3\"]').style(\"fill\", colors[order[2]]).style(\"opacity\", 0.33);\n            d3.select(to).selectAll('g[*|label*=\"z_r1\"]').selectChildren(\"rect\").style(\"fill\", colors[order[0]]).style(\"opacity\", 0.33);\n            d3.select(to).selectAll('g[*|label*=\"z_r2\"]').selectChildren(\"rect\").style(\"fill\", colors[order[1]]).style(\"opacity\", 0.33);\n            d3.select(to).selectAll('g[*|label*=\"z_r3\"]').selectChildren(\"rect\").style(\"fill\", colors[order[2]]).style(\"opacity\", 0.33);\n            d3.select(to).selectAll('g[*|label*=\"v_r1\"]').selectChildren(\"rect\").style(\"fill\", colors[order[0]]).style(\"opacity\", 0.33);\n            d3.select(to).selectAll('g[*|label*=\"v_r2\"]').selectChildren(\"rect\").style(\"fill\", colors[order[1]]).style(\"opacity\", 0.33);\n            d3.select(to).selectAll('g[*|label*=\"v_r3\"]').selectChildren(\"rect\").style(\"fill\", colors[order[2]]).style(\"opacity\", 0.33);\n        };\n\n        AttnHover(to);\n        });\n};\n\nfunction drawShuffle(X, P, words, Wq, Wk, Wv, order, from, to, add_P) {\n    d3.xml(from)\n    .then(data => {\n        if (d3.select(to).node().children.length == 0) {\n            d3.select(to).node().append(data.documentElement);\n        }\n        d3.select(to).selectAll('text').style(\"cursor\", \"default\");\n\n        var shuffle = d3.select(to).select('g[*|label=\"shuffle\"]');\n        shuffle.on(\"mouseover\", function(d) {\n            d3.select(this).selectChildren(\"path\").style(\"stroke\", \"#ffb482\").style(\"opacity\", 0.66);\n        });\n        shuffle.on(\"mouseout\", function(d) {\n            d3.select(this).selectChildren(\"path\").style(\"stroke\", \"black\").style(\"opacity\", 1.00);\n        });\n\n        shuffle.on('click', function() {\n            shuffleArray(order);\n            var [Q,K,V,QK,A,Z] = compute(order.map(i=>X[i]), P, Wq, Wk, Wv, add_P);\n            drawX(order.map(i=>X[i]), order.map(i=>words[i]), order,from, to, true);\n            drawQKV(Q,K,V,Wq,Wk,Wv,order,from, to, true);\n            drawAttn(Q,K,V,QK,A,Z,order,from, to, true);\n        });\n    });\n};\n\nfunction drawP(P, from, to) {\n    d3.xml(from)\n    .then(data => {\n        if (d3.select(to).node().children.length == 0) {\n            d3.select(to).node().append(data.documentElement);\n        }\n\n        fillmatrix(P, [\"11\", \"12\", \"21\", \"22\", \"31\", \"32\"], \"p\", to);\n        d3.select(to).selectAll('text').style(\"cursor\", \"default\");\n        d3.select(to).selectAll('rect[*|label*=\"P\"]').style(\"fill\", '#d0bbff').style(\"opacity\", 0.66);\n\n        });\n};\n\nfunction drawRotation(Q,K,Qrot,Krot,start_index, order, from, to) {\n    d3.xml(from)\n    .then(data => {\n        if (d3.select(to).node().children.length == 0) {\n            d3.select(to).node().append(data.documentElement);\n        }\n\n        d3.select(to).selectAll('text').style(\"cursor\", \"default\");\n\n        var colors = [\"#ffb482\", \"#8de5a1\", \"#ff9f9b\"];\n        d3.select(to).selectAll('rect[*|label*=\"Q1\"]').style(\"fill\", colors[order[0]]).style(\"opacity\", 0.33);\n        d3.select(to).selectAll('rect[*|label*=\"Q2\"]').style(\"fill\", colors[order[1]]).style(\"opacity\", 0.33);\n        d3.select(to).selectAll('rect[*|label*=\"Q3\"]').style(\"fill\", colors[order[2]]).style(\"opacity\", 0.33);\n        d3.select(to).selectAll('rect[*|label*=\"K1\"]').style(\"fill\", colors[order[0]]).style(\"opacity\", 0.33);\n        d3.select(to).selectAll('rect[*|label*=\"K2\"]').style(\"fill\", colors[order[1]]).style(\"opacity\", 0.33);\n        d3.select(to).selectAll('rect[*|label*=\"K3\"]').style(\"fill\", colors[order[2]]).style(\"opacity\", 0.33);\n\n        const pos = [start_index, start_index+1, start_index+2];\n        var format_rotmat = function(p) {\n            return [[\"cos(\"+p+\")\", \"-sin(\"+p+\")\"], [\"sin(\"+p+\")\", \"cos(\"+p+\")\"]]\n        }\n\n        fillmatrix(Q, [\"11\", \"12\", \"21\", \"22\", \"31\", \"32\"], \"Q\", to);\n        fillmatrix(K, [\"11\", \"12\", \"21\", \"22\", \"31\", \"32\"], \"K\", to);\n\n        fillmatrix(format_rotmat(pos[0]), [\"11\", \"12\", \"21\", \"22\"], \"R1\", to);\n        fillmatrix(format_rotmat(pos[1]), [\"11\", \"12\", \"21\", \"22\"], \"R2\", to);\n        fillmatrix(format_rotmat(pos[2]), [\"11\", \"12\", \"21\", \"22\"], \"R3\", to);\n        \n        fillmatrix_formatted(Qrot, [\"11\", \"12\", \"21\", \"22\", \"31\", \"32\"], \"Qrot\", \".2f\", to);\n        fillmatrix_formatted(Krot, [\"11\", \"12\", \"21\", \"22\", \"31\", \"32\"], \"Krot\", \".2f\", to);\n        \n    });\n};\n\nconst words = [\"love\", \"conquers\", \"all\"];\nvar X = [[1, 2], [0, 2], [1, 1]];\nconst Wq = [ [1,-1],[0,1] ];\nconst Wk = [ [1,0],[1,-1] ];\nconst Wv = [ [1,0],[1,1] ];\nvar order = [0,1,2];\n\n\nvar [Q,K,V,QK,A,Z] = compute(X, 0, Wq, Wk, Wv, false);\n\ndrawX(X, words, order,'./x.svg', '#vis-x');\ndrawQKV(Q,K,V,Wq,Wk,Wv,order,'./qkv.svg', '#vis-qkv');\n\ndrawAttn(Q,K,V,QK,A,Z,order,'./attn.svg', '#vis-attn');\n\ndrawX(X, words, order,'./shuffler.svg', '#vis-shuffle', true);\ndrawQKV(Q,K,V,Wq,Wk,Wv,order,'./shuffler.svg', '#vis-shuffle', true);\ndrawAttn(Q,K,V,QK,A,Z,order,'./shuffler.svg', '#vis-shuffle', true);\ndrawShuffle(X, 0, words, Wq, Wk, Wv, order, './shuffler.svg', '#vis-shuffle', false);\n\nconst P = [[0,0], [1, 1], [2, 2]];\nvar [Q,K,V,QK,A,Z] = compute(X, P, Wq, Wk, Wv, true);\ndrawX(X, words, order,'./pos.svg', '#vis-pos', true);\ndrawP(P, './pos.svg', '#vis-pos')\ndrawQKV(Q,K,V,Wq,Wk,Wv,order,'./pos.svg', '#vis-pos', true);\ndrawAttn(Q,K,V,QK,A,Z,order,'./pos.svg', '#vis-pos', true);\ndrawShuffle(X, P, words, Wq, Wk, Wv, order, './pos.svg', '#vis-pos', true);\n\nconst paragraph = [\"Lorem\", \"ipsum\", \"dolor\", \"sit\", \"amet,\", \"consectetur\", \"adipiscing\", \"elit,\", \"sed\", \"do\", \"eiusmod\", \"tempor\", \"incididunt\", \"ut\", \"labore\", \"et\", \"dolore\", \"magna\", \"aliqua.\", \"Ut\", \"enim\", \"ad\", \"minim\", \"veniam,\", \"quis\", \"nostrud\", \"exercitation\", \"ullamco\", \"laboris\", \"nisi\", \"ut\", \"aliquip\", \"ex\", \"ea\", \"commodo\", \"consequat.</small></small>\"];\n\nconst insertion = [\"</small></small><b>love\", \"conquers\", \"all</b><small><small>\"];\nvar location = document.getElementById(\"slider\").value;\n\nfunction displayParagraph(paragraph_id, location) {\n    const joined = [\"<small><small>\"].concat(paragraph.slice(0, location),insertion, paragraph.slice(location, -1));\n    var div = document.getElementById(paragraph_id);\n    div.innerHTML = joined.join(\" \");\n};\n\ndisplayParagraph('paragraph', 5);\ndisplayParagraph('paragraph2', location);\n\n\nvar [Q,K,V,Qrot,Krot,QK,A,Z] = computeRotary(X, 0, Wq, Wk, Wv, +location);\ndrawX(X, words, order,'./x.svg', '#vis-x-rel', true);\ndrawQKV(Q,K,V,Wq,Wk,Wv,order,'./qkv.svg', '#vis-qkv-rel', true);\ndrawRotation(Q,K,Qrot,Krot,+location,order,'./rotary.svg', '#vis-rot-rel');\ndrawAttn(Qrot,Krot,V,QK,A,Z,order,'./attn.svg', '#vis-attn-rel', true, true);\n\nd3.select(\"#slider\").on(\"change\", function(d){\n    location = this.value\n    displayParagraph('paragraph2', location);\n    order = [0,1,2];\n    var [Q,K,V,Qrot,Krot,QK,A,Z] = computeRotary(X, 0, Wq, Wk, Wv, +location);\n    drawRotation(Q,K,Qrot,Krot,+location,order,'./rotary.svg', '#vis-rot-rel');\n    drawAttn(Qrot,Krot,V,QK,A,Z,order,'./attn.svg', '#vis-attn-rel', true, true);\n  });\n\n</script>\n<hr><h2 class=\"mt-3\">References and footnotes</h2>\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn1\" class=\"footnote-item\"><p>For more info on what I mean with geometric priors, refer to <a href=\"https://geometricdeeplearning.com/\">the geometric deep learning book</a>. <a href=\"https://gdewael.github.io/blog/equivariance/\" class=\"footnote-backref\">↩︎</a></p>\n</li>\n<li id=\"fn2\" class=\"footnote-item\"><p>If you are not familiar with transformers, consider reading <a href=\"http://jalammar.github.io/illustrated-transformer/\">The Illustrated Transformer by Jay Alammar</a> <a href=\"https://gdewael.github.io/blog/equivariance/\" class=\"footnote-backref\">↩︎</a></p>\n</li>\n<li id=\"fn3\" class=\"footnote-item\"><p>Vaswani, Ashish, et al. &quot;Attention is all you need.&quot; Advances in neural information processing systems 30 (2017). <a href=\"https://gdewael.github.io/blog/equivariance/\" class=\"footnote-backref\">↩︎</a></p>\n</li>\n<li id=\"fn4\" class=\"footnote-item\"><p>Devlin, Jacob, et al. &quot;Bert: Pre-training of deep bidirectional transformers for language understanding.&quot; arXiv preprint arXiv:1810.04805 (2018). <a href=\"https://gdewael.github.io/blog/equivariance/\" class=\"footnote-backref\">↩︎</a></p>\n</li>\n<li id=\"fn5\" class=\"footnote-item\"><p>Su, Jianlin, et al. &quot;Roformer: Enhanced transformer with rotary position embedding.&quot; Neurocomputing 568 (2024): 127063. <a href=\"https://gdewael.github.io/blog/equivariance/\" class=\"footnote-backref\">↩︎</a></p>\n</li>\n<li id=\"fn6\" class=\"footnote-item\"><p>Fuchs, Fabian, et al. &quot;Se (3)-transformers: 3d roto-translation equivariant attention networks.&quot; Advances in neural information processing systems 33 (2020): 1970-1981. <a href=\"https://gdewael.github.io/blog/equivariance/\" class=\"footnote-backref\">↩︎</a></p>\n</li>\n</ol>\n</section>\n",
			"date_published": "2024-05-20T00:00:00Z"
		}
		,
		{
			"id": "https://gdewael.github.io/blog/travelmapping/",
			"url": "https://gdewael.github.io/blog/travelmapping/",
			"title": "Mapping travels with Folium",
			"content_html": "<p>There's a map at my grandparents' home containing sewing pins that indicate the places they have traveled.\nAs a kid, I used to gawk at this map and wonder how these places looked like.\nAs an adult, I have come to appreciate this map as a testament to a life well-spent traveling together.</p>\n<p style=\"text-align: center;\">\n  <img alt=\"A map\" loading=\"lazy\" decoding=\"async\" src=\"https://gdewael.github.io/img/G7zcRw7hlV-700.png\" width=\"700\" height=\"393\">\n  <br>\n  <i>The map at my grandparents' house. Every sewing pin indicates a visited place.</i>\n</p>\n<p>As an homage to their map (and to satisfy my own love for staring at maps and tracking data), I have decided to similarly start tracking travels.\nInstead of using sewing pins, I will be hosting it on this site.\nCurrently, the map contains only multi-day trails.\nI may expand this with dots for places visited using more-conventional means of travel, but for the time being, I'm content with how it looks.\nThe result is:</p>\n<p style=\"text-align: center;\">\n  <iframe src=\"https://gdewael.github.io/vis/treks/index.html\" width=\"700\" height=\"450\"></iframe>\n  <br>\n  <i>A map containing my multi-day trails</i>\n</p>\n<p>As the map itself is very much a work in progress, the remainder of this page will outline how I created the map, and how I integrated it with this site.</p>\n<h2 id=\"overview-and-file-prerequisites\" tabindex=\"-1\">Overview and file prerequisites <a class=\"header-anchor\" href=\"https://gdewael.github.io/blog/travelmapping/\">#</a></h2>\n<p>The setup consists of a python file that (1) reads <em>.gpx</em> and <em>.jpg</em> files, and (2) renders it on a Leaflet.js map using <a href=\"https://python-visualization.github.io/folium/latest/\">folium</a>.\nTo get <em>.gpx</em> files for hikes, I use the <a href=\"https://www.outdooractive.com/en/\">Outdooractive</a> platform, which is also my mobile app during hikes.\nWith my current workflow, the folder structure looks as follows:</p>\n<pre class=\"language-bash\" tabindex=\"0\"><code class=\"language-bash\">map/\n├── generate_map.py\n├── mapping.json\n├── requirements.txt\n├── img\n│   └── *.jpg\n└── tracks\n    └── *.gpx</code></pre>\n<p>In a <em>mapping.json</em> file, I can record some metadata for every track:</p>\n<pre class=\"language-bash\" tabindex=\"0\"><code class=\"language-bash\"><span class=\"token operator\">></span> <span class=\"token function\">cat</span> mapping.json\n<span class=\"token punctuation\">{</span>\n    <span class=\"token string\">\"camino_primitivo.gpx\"</span><span class=\"token builtin class-name\">:</span> <span class=\"token punctuation\">[</span><span class=\"token string\">\"12-day Camino Primitivo\"</span>, <span class=\"token string\">\"August 2015\"</span>, <span class=\"token string\">\"camino.JPG\"</span>, <span class=\"token string\">\"multi-day\"</span><span class=\"token punctuation\">]</span>,\n    <span class=\"token punctuation\">..</span>.\n    <span class=\"token string\">\"tarn.gpx\"</span><span class=\"token builtin class-name\">:</span> <span class=\"token punctuation\">[</span><span class=\"token string\">\"2-day Packrafting of the Tarn\"</span>, <span class=\"token string\">\"July 2023\"</span>, <span class=\"token string\">\"tarn.jpg\"</span>, <span class=\"token string\">\"packraft\"</span><span class=\"token punctuation\">]</span>\n<span class=\"token punctuation\">}</span></code></pre>\n<p>Currently, for every <em>.gpx</em> file, I'm tracking a short description of the trail, a description of the date, an image file, and what kind of trail it corresponds to.\nAll of these attributes are used in the tooltips of the trails on the map.</p>\n<p>In order to add new tracks, I add a <em>.gpx</em> track, an image, and a new entry in the <em>mapping.json</em> file.\nIn terms of ease-of-use, it's not quite up-to-par with sticking a sewing pin on a paper map, but let's hope it stands the test of time.</p>\n<h2 id=\"mapping-with-folium\" tabindex=\"-1\">Mapping with Folium <a class=\"header-anchor\" href=\"https://gdewael.github.io/blog/travelmapping/\">#</a></h2>\n<p>The script is kept deliberately simple.\nIt creates a Folium map:</p>\n<details><summary>Code for folium map</summary>\n<pre class=\"language-python\" tabindex=\"0\"><code class=\"language-python\">m <span class=\"token operator\">=</span> folium<span class=\"token punctuation\">.</span>Map<span class=\"token punctuation\">(</span>location <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token number\">51.057056</span><span class=\"token punctuation\">,</span> <span class=\"token number\">3.702139</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> zoom_start <span class=\"token operator\">=</span> <span class=\"token number\">4</span><span class=\"token punctuation\">,</span> tiles<span class=\"token operator\">=</span><span class=\"token string\">\"CartoDB dark_matter\"</span><span class=\"token punctuation\">)</span>\n\nFullscreen<span class=\"token punctuation\">(</span>\n\t\tposition<span class=\"token operator\">=</span><span class=\"token string\">\"topright\"</span><span class=\"token punctuation\">,</span>\n\t\ttitle<span class=\"token operator\">=</span><span class=\"token string\">\"Expand me\"</span><span class=\"token punctuation\">,</span>\n\t\ttitle_cancel<span class=\"token operator\">=</span><span class=\"token string\">\"Exit me\"</span><span class=\"token punctuation\">,</span>\n\t\tforce_separate_button<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">,</span>\n<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>add_to<span class=\"token punctuation\">(</span>m<span class=\"token punctuation\">)</span></code></pre>\n</details>\n<p>After which it loops over the entries in the metadata <em>mapping.json</em> files:</p>\n<details><summary>Code for adding tracks to map</summary>\n<pre class=\"language-python\" tabindex=\"0\"><code class=\"language-python\">colors <span class=\"token operator\">=</span> sns<span class=\"token punctuation\">.</span>color_palette<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>as_hex<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\ncolor_mapping <span class=\"token operator\">=</span> <span class=\"token punctuation\">{</span>\n\t\t<span class=\"token string\">\"multi-day\"</span> <span class=\"token punctuation\">:</span> colors<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n\t\t<span class=\"token string\">\"packraft\"</span> <span class=\"token punctuation\">:</span> colors<span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n\t\t<span class=\"token string\">\"day trip\"</span> <span class=\"token punctuation\">:</span> colors<span class=\"token punctuation\">[</span><span class=\"token number\">2</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n<span class=\"token punctuation\">}</span>\n\n<span class=\"token keyword\">with</span> <span class=\"token builtin\">open</span><span class=\"token punctuation\">(</span>mapping_json_path<span class=\"token punctuation\">)</span> <span class=\"token keyword\">as</span> json_file<span class=\"token punctuation\">:</span>\n\t\tdata <span class=\"token operator\">=</span> json<span class=\"token punctuation\">.</span>load<span class=\"token punctuation\">(</span>json_file<span class=\"token punctuation\">)</span>\n\n\n<span class=\"token keyword\">for</span> ix<span class=\"token punctuation\">,</span> l <span class=\"token keyword\">in</span> <span class=\"token builtin\">enumerate</span><span class=\"token punctuation\">(</span>os<span class=\"token punctuation\">.</span>listdir<span class=\"token punctuation\">(</span>gpx_folder_path<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n\t\t<span class=\"token keyword\">if</span> l<span class=\"token punctuation\">.</span>endswith<span class=\"token punctuation\">(</span><span class=\"token string\">\".gpx\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n\t\t\t\tname<span class=\"token punctuation\">,</span> date<span class=\"token punctuation\">,</span> pic<span class=\"token punctuation\">,</span> color <span class=\"token operator\">=</span> data<span class=\"token punctuation\">[</span>l<span class=\"token punctuation\">]</span>\n\n\t\t\t\tcoords <span class=\"token operator\">=</span> load_gpx<span class=\"token punctuation\">(</span>os<span class=\"token punctuation\">.</span>path<span class=\"token punctuation\">.</span>join<span class=\"token punctuation\">(</span>gpx_folder_path<span class=\"token punctuation\">,</span> l<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n\t\t\t\ttooltip <span class=\"token operator\">=</span> <span class=\"token string\">\"%s (%s)\"</span> <span class=\"token operator\">%</span> <span class=\"token punctuation\">(</span>name<span class=\"token punctuation\">,</span> date<span class=\"token punctuation\">)</span>\n\t\t\t\tpopup <span class=\"token operator\">=</span> img_to_thumbnail_popup<span class=\"token punctuation\">(</span>os<span class=\"token punctuation\">.</span>path<span class=\"token punctuation\">.</span>join<span class=\"token punctuation\">(</span>img_folder_path<span class=\"token punctuation\">,</span> pic<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> tooltip<span class=\"token punctuation\">)</span>\n\n\t\t\t\t<span class=\"token comment\"># Outline</span>\n\t\t\t\tfolium<span class=\"token punctuation\">.</span>PolyLine<span class=\"token punctuation\">(</span>\n\t\t\t\t\t\tcoords<span class=\"token punctuation\">,</span> weight<span class=\"token operator\">=</span><span class=\"token number\">8</span><span class=\"token punctuation\">,</span> color <span class=\"token operator\">=</span> <span class=\"token string\">\"white\"</span><span class=\"token punctuation\">,</span>\n\t\t\t\t<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>add_to<span class=\"token punctuation\">(</span>m<span class=\"token punctuation\">)</span>\n\n\t\t\t\t<span class=\"token comment\"># Colored line</span>\n\t\t\t\tfolium<span class=\"token punctuation\">.</span>PolyLine<span class=\"token punctuation\">(</span>\n\t\t\t\t\t\tcoords<span class=\"token punctuation\">,</span> weight<span class=\"token operator\">=</span><span class=\"token number\">6</span><span class=\"token punctuation\">,</span> color <span class=\"token operator\">=</span> color_mapping<span class=\"token punctuation\">[</span>color<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n\t\t\t\t\t\ttooltip<span class=\"token operator\">=</span>tooltip<span class=\"token punctuation\">,</span>\n\t\t\t\t\t\tpopup<span class=\"token operator\">=</span>popup<span class=\"token punctuation\">,</span>\n\t\t\t\t<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>add_to<span class=\"token punctuation\">(</span>m<span class=\"token punctuation\">)</span>\n\nm<span class=\"token punctuation\">.</span>save<span class=\"token punctuation\">(</span>output_file<span class=\"token punctuation\">)</span></code></pre>\n</details>\n<p>This piece of code makes use of a function <em>load_gpx</em>, that loads <em>.gpx</em> files to pandas DataFrames:</p>\n<details><summary>Code for reading gpx files</summary>\n<pre class=\"language-python\" tabindex=\"0\"><code class=\"language-python\"><span class=\"token keyword\">def</span> <span class=\"token function\">load_gpx</span><span class=\"token punctuation\">(</span>file_path<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">with</span> <span class=\"token builtin\">open</span><span class=\"token punctuation\">(</span>file_path<span class=\"token punctuation\">)</span> <span class=\"token keyword\">as</span> f<span class=\"token punctuation\">:</span>\n            gpx <span class=\"token operator\">=</span> gpxpy<span class=\"token punctuation\">.</span>parse<span class=\"token punctuation\">(</span>f<span class=\"token punctuation\">)</span>\n\n    <span class=\"token comment\"># Convert to a dataframe one point at a time.</span>\n    points <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>\n    <span class=\"token keyword\">for</span> segment <span class=\"token keyword\">in</span> gpx<span class=\"token punctuation\">.</span>tracks<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>segments<span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">for</span> p <span class=\"token keyword\">in</span> segment<span class=\"token punctuation\">.</span>points<span class=\"token punctuation\">:</span>\n            points<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span><span class=\"token punctuation\">{</span>\n                <span class=\"token string\">'time'</span><span class=\"token punctuation\">:</span> p<span class=\"token punctuation\">.</span>time<span class=\"token punctuation\">,</span>\n                <span class=\"token string\">'latitude'</span><span class=\"token punctuation\">:</span> p<span class=\"token punctuation\">.</span>latitude<span class=\"token punctuation\">,</span>\n                <span class=\"token string\">'longitude'</span><span class=\"token punctuation\">:</span> p<span class=\"token punctuation\">.</span>longitude<span class=\"token punctuation\">,</span>\n                <span class=\"token string\">'elevation'</span><span class=\"token punctuation\">:</span> p<span class=\"token punctuation\">.</span>elevation<span class=\"token punctuation\">,</span>\n            <span class=\"token punctuation\">}</span><span class=\"token punctuation\">)</span>\n    df <span class=\"token operator\">=</span> pd<span class=\"token punctuation\">.</span>DataFrame<span class=\"token punctuation\">.</span>from_records<span class=\"token punctuation\">(</span>points<span class=\"token punctuation\">)</span>\n    coords <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">(</span>i<span class=\"token punctuation\">,</span> j<span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span> i<span class=\"token punctuation\">,</span> j <span class=\"token keyword\">in</span> <span class=\"token builtin\">zip</span><span class=\"token punctuation\">(</span>df<span class=\"token punctuation\">[</span><span class=\"token string\">\"latitude\"</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> df<span class=\"token punctuation\">[</span><span class=\"token string\">\"longitude\"</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span>\n    <span class=\"token keyword\">return</span> coords</code></pre>\n</details>\n<p>In addition, pictures are processed using a <em>img_to_thumbnail_popup</em> function.\nOf note here is that the images in the popups are programmed to be Base64 encoded and served inline.\nThis is the quickest way I've gotten it set-up, but I realize that this will need to be changed in the future for scalability reasons.</p>\n<details><summary>Code for creating thumbnails</summary>\n<pre class=\"language-python\" tabindex=\"0\"><code class=\"language-python\"><span class=\"token keyword\">def</span> <span class=\"token function\">img_to_thumbnail_popup</span><span class=\"token punctuation\">(</span>file_path<span class=\"token punctuation\">,</span> tooltip<span class=\"token punctuation\">,</span> size <span class=\"token operator\">=</span> <span class=\"token number\">300</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token builtin\">buffer</span> <span class=\"token operator\">=</span> io<span class=\"token punctuation\">.</span>BytesIO<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    img <span class=\"token operator\">=</span> Image<span class=\"token punctuation\">.</span><span class=\"token builtin\">open</span><span class=\"token punctuation\">(</span>file_path<span class=\"token punctuation\">)</span>\n    img<span class=\"token punctuation\">.</span>thumbnail<span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>size<span class=\"token punctuation\">,</span> size<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\"># x, y</span>\n    img<span class=\"token punctuation\">.</span>save<span class=\"token punctuation\">(</span><span class=\"token builtin\">buffer</span><span class=\"token punctuation\">,</span> <span class=\"token builtin\">format</span><span class=\"token operator\">=</span><span class=\"token string\">\"jpeg\"</span><span class=\"token punctuation\">)</span>\n    encoded <span class=\"token operator\">=</span> base64<span class=\"token punctuation\">.</span>b64encode<span class=\"token punctuation\">(</span><span class=\"token builtin\">buffer</span><span class=\"token punctuation\">.</span>getvalue<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n    html <span class=\"token operator\">=</span> <span class=\"token string\">'%s&lt;p>&lt;img src=\"data:image/png;base64,%s\">'</span> <span class=\"token operator\">%</span> <span class=\"token punctuation\">(</span>tooltip<span class=\"token punctuation\">,</span> encoded<span class=\"token punctuation\">.</span>decode<span class=\"token punctuation\">(</span><span class=\"token string\">'UTF-8'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    iframe <span class=\"token operator\">=</span> branca<span class=\"token punctuation\">.</span>element<span class=\"token punctuation\">.</span>IFrame<span class=\"token punctuation\">(</span>html<span class=\"token operator\">=</span>html<span class=\"token punctuation\">,</span> width<span class=\"token operator\">=</span><span class=\"token number\">325</span><span class=\"token punctuation\">,</span> height <span class=\"token operator\">=</span> <span class=\"token number\">325</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">return</span> folium<span class=\"token punctuation\">.</span>Popup<span class=\"token punctuation\">(</span>iframe<span class=\"token punctuation\">,</span> max_width<span class=\"token operator\">=</span><span class=\"token number\">325</span><span class=\"token punctuation\">)</span></code></pre>\n</details>\n<p>The full script can be found <a href=\"https://github.com/gdewael/gdewael.github.io/blob/main/map/generate_map.py\">here</a>.</p>\n<h2 id=\"integration-with-github-pages-static-site\" tabindex=\"-1\">Integration with GitHub Pages static site <a class=\"header-anchor\" href=\"https://gdewael.github.io/blog/travelmapping/\">#</a></h2>\n<p>The script writes the map to a <em>.html</em> file, which can be readily used in my static site, using iframes, for example.\nI have the Eleventy build of this site hooked up to GitHub Actions, so why not do the same for this map?\nThis way, I don't have to run the python script manually every time I add a new trail.\nTo do this, I add the following &quot;steps&quot; in <a href=\"https://github.com/gdewael/gdewael.github.io/blob/main/.github/workflows/build-and-deploy.yml\">my GitHub Actions workflow file</a>:</p>\n<pre class=\"language-yml\" tabindex=\"0\"><code class=\"language-yml\"><span class=\"token punctuation\">-</span> <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> setup python\n\t<span class=\"token key atrule\">uses</span><span class=\"token punctuation\">:</span> actions/setup<span class=\"token punctuation\">-</span>python@v4\n\t<span class=\"token key atrule\">with</span><span class=\"token punctuation\">:</span>\n\t\t<span class=\"token key atrule\">python-version</span><span class=\"token punctuation\">:</span> <span class=\"token string\">'3.12'</span> <span class=\"token comment\"># install the python version needed</span>\n\n<span class=\"token punctuation\">-</span> <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> install python packages\n  <span class=\"token key atrule\">run</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">|</span><span class=\"token scalar string\">\n\t\tpython -m pip install --upgrade pip\n\t\tpip install -r ./map/requirements.txt</span>\n\n<span class=\"token punctuation\">-</span> <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> execute py script <span class=\"token comment\"># run main.py</span>\n\t<span class=\"token key atrule\">run</span><span class=\"token punctuation\">:</span> python ./map/generate_map.py <span class=\"token punctuation\">-</span><span class=\"token punctuation\">-</span>mapping_json_path map/mapping.json <span class=\"token punctuation\">-</span><span class=\"token punctuation\">-</span>img_folder_path map/img/ <span class=\"token punctuation\">-</span><span class=\"token punctuation\">-</span>gpx_folder_path map/tracks/ <span class=\"token punctuation\">-</span><span class=\"token punctuation\">-</span>output_file content/treks.html\n</code></pre>\n<p>To add a new trail, I just push a new entry in the <em>metadata.json</em> file, along with adding an image and gpx file, to GitHub.\nUpon doing so, the workflow triggers, automatically updating the map and the site along with it.</p>\n<p>I will be hosting a stand-alone page of the map <a href=\"https://gdewael.github.io/map\">here</a>.</p>\n",
			"date_published": "2024-01-18T00:00:00Z"
		}
		
	]
}
